## Distance Calculation From a Camera

Distance calculation is one important problem that must be treated in autonomous
driving systems and for road safety. It is also useful in computer vision
applications such as animal conservation. Instead of talking about road safety
which is largely treated in the computer vision industry, let us treat this
problem under the prism of animal conservation to bring justice to environmental
and wildlife topics.

For example, let us imagine that we are biologists or animal conservationists
aiming at counting a colony of penguins resting on a large ice floe. We are
flying a drone equipped with multiple cameras and with IMU sensors so that we
can estimate the altitude of the drone at anytime. The drone embeds a robust
software that is able to recognize penguins and track them.

For a robust counting of penguins, we aim at mapping their positions in 3D by
determining the metric coordinates of each penguin. The embedded software is
able to localize penguins and detect their feet in the images. Then knowing the
pixel coordinates of each penguin's feet, we are able to determine the metric
coordinates of each penguin in the ice floe.

This section provides an easily implementable method to calculate distance from
a camera. Calculating distances from a camera requires the knowledge of the
internal camera parameters. Whenever possible, we will use normalized camera
coordinates, that is the camera film coordinates, instead of pixel coordinates.

Throughout this section, the main assumption we make is that the ice flow is
planar.

### Ideal Case

The drone is flying and it is staying still in the air at an altitude $h$
meters. The drone is equipped with a front camera in upright position at the
ground looking to the horizon, that is,

- its $y$-axis is normal to the ground plane and,
- its $x$-axis and $z$-axis forms a plane that is parallel to the ground plane.

The front camera sees penguins. Let $M$ denote a point on the ground at a
penguin's feet position. Its coordinates in the camera frame are

\begin{equation}
  M = x\ \mathbf{i} + h\ \mathbf{j} + z\ \mathbf{k}
\end{equation}

The point $M$ is visible on the camera. Let $(u, v)$ denote its normalized
camera coordinates.

By similar triangles it follows that

\begin{equation}
  \frac{u}{x} = \frac{v}{h} = \frac{1}{z}
\end{equation}

Thus the metric coordinates of a penguin are:

\begin{equation}
  \left\{
  \begin{array}{ccl}
    x &=& \frac{u}{v} h \\
    y &=& h \\
    z &=& \frac{1}{v} h
  \end{array}
  \right.
\end{equation}

### A More Motivating Example: Nonzero Pitch Angle

In real life, the drone does not necessarily have a perfect a zero pitch angle.
It could be that because of the wind the drone has to maintain its stationary
position with a nonzero pitch angle. We show here how we could calculate
distances between penguins and the drone from the less ideal case.

This section is a prelude to the next section which presents a more general
approach.

Let us imagine that the front camera is at a fixed height $h$ above the ground
slight looking down due to a nonzero pitch angle $\theta$. Notice that in the
camera frame the pitch angle is negative.

Now also consider an imaginary camera placed at the same camera position and
this time gazing at the horizon with a zero pitch angle. The virtual camera
frame will be considered the reference frame. This virtual camera corresponds to
the ideal case we described in the previous section.

Let $M$ denote a point on the ground. Its coordinates in the reference frame are
still

\begin{equation}
  M = x\ \mathbf{i} + h\ \mathbf{j} + z\ \mathbf{k}
\end{equation}

The basis vectors of the reference frame have the following
coordinates in the actual camera frame

\begin{equation}
  \begin{array}{ccl}
  \mathbf{i} &=&  \mathbf{i}_C \\
  \mathbf{j} &=&  \cos\theta\ \mathbf{j}_C + \sin\theta\ \mathbf{k}_C \\
  \mathbf{k} &=& -\sin\theta\ \mathbf{j}_C + \cos\theta\ \mathbf{k}_C
  \end{array}
\end{equation}

Rewriting the coordinates of $M$ in the camera frame

\begin{equation}
  \begin{array}{ccl}
  M &=& x\ \mathbf{i}_C +
        h (\cos\theta\ \mathbf{j}_C + \sin\theta\ \mathbf{k}_C) +
        z (-\sin\theta\ \mathbf{j}_C + \cos\theta\ \mathbf{k}_C) \\

  M &=& x\ \mathbf{i}_C +
        (h \cos\theta - z \sin\theta)\ \mathbf{j}_C +
        (h \sin\theta + z\cos\theta)\ \mathbf{k}_C \\
  \end{array}
\end{equation}

Thus in the camera frame

\begin{equation}
  \begin{array}{ccl}
  x_C &=& x \\
  y_C &=& h \cos\theta - z \sin\theta \\
  z_C &=& h \sin\theta + z\cos\theta \\
  \end{array}
\end{equation}

The point $M$ is projected to the normalized camera plane. Let $(u,
v)$ denote its normalized camera coordinates.

By similar triangles it follows that

\begin{equation}
  \frac{u}{x_C} = \frac{v}{y_C} = \frac{1}{z_C}
\end{equation}


We can inject the equations to calculate $z$:

\begin{align*}
  v &= \frac{y_C}{z_C} \\
  v &= \frac{h \cos\theta - z \sin\theta}{h \sin\theta + z\cos\theta} \\
  v (h \sin\theta + z\cos\theta) &= h \cos\theta - z \sin\theta \\
  z (v\cos\theta + \sin\theta) &= h (\cos\theta - v \sin\theta)
\end{align*}

In the end:

\begin{equation}
  z = h \frac{\cos\theta - v \sin\theta}{\sin\theta + v\cos\theta} \\
\end{equation}

### Generalization to Any Global Rigid Transform

We can generalize elegantly the reasoning by means of linear algebra. And it
will not matter which angular direction $(\psi, \theta, \phi)$ the camera
is looking at.

We still assume that the road is *planar*. Following the usual convention in the
automotive industry, without loss of generality, any point on the ground is at
height $z = 0$. Note this observation stays valid whether the vehicle
climbs or descends does not matter as long as the road is *planar*.

Suppose that we know the camera pose parameterised by the rigid body transform
$(\mathbf{R}, \mathbf{t})$ w.r.t. the vehicle frame.

As said earlier, a ground point $M$ has more natural coordinates $\mathbf{x} =
(x, y, 0)$ in the vehicle frame. Let $\mathbf{x}'= (x', y', z')$ denote its
coordinates in the camera frame. The rigid body transform relates the two vector
quantities as follows

\begin{equation}
  \mathbf{x} = \mathbf{R} \mathbf{x}' + \mathbf{t} \\
\end{equation}

The inverse rigid body transform is $(\mathbf{R}', \mathbf{t}')$
where:

\begin{equation}
  \begin{aligned}
  \mathbf{R}' &= \mathbf{R}^T \\
  \mathbf{t}' &= -\mathbf{R}^T \mathbf{t} \\
  \mathbf{x}' &= \mathbf{R}' \mathbf{x} + \mathbf{t}' \\
  \end{aligned}
\end{equation}

If the ground point is visible in the image at the following normalized
coordinates $(u, v)$, then using the basic proportionality theorem in
geometric optics:

\begin{equation}
  \frac{u}{x'} = \frac{v}{y'} = \frac{1}{z'},
\end{equation}

We can derive a system of two equations.

\begin{equation}
  \left\{ \begin{array}{lll}
  u z' - x' &=& 0 \\
  v z' - y' &=& 0 \\
  \end{array} \right.
\end{equation}

Expanding the matrix operation into a linear system:

\begin{equation}
  \mathbf{x}' = \mathbf{R}' \mathbf{x} + \mathbf{t}' \\
\end{equation}

yields

\begin{equation}
  \left\{ \begin{array}{lll}
  x' &=& r'_{11} x + r'_{12} y + r'_{13} z + t'_{1}\\
  y' &=& r'_{21} x + r'_{22} y + r'_{23} z + t'_{2}\\
  z' &=& r'_{31} x + r'_{32} y + r'_{33} z + t'_{3}\\
  \end{array} \right.
\end{equation}

Injecting these equations in the system of two equations yields

\begin{equation}
  \left\{ \begin{array}{lll}
  u (r'_{31} x + r'_{32} y + r'_{33} z + t'_{3}) -
    (r'_{11} x + r'_{12} y + r'_{13} z + t'_{1})  &=& 0 \\

  v (r'_{31} x + r'_{32} y + r'_{33} z + t'_{3}) -
    (r'_{21} x + r'_{22} y + r'_{23} z + t'_{2}) &=& 0\\
  \end{array} \right.
\end{equation}

Reordering

\begin{equation}
  \left\{ \begin{array}{lll}
  (u r'_{31} - r'_{11}) x - (u r'_{32} - r'_{12}) y + (u r'_{33} - r'_{13}) z
  &=& t'_{1} - u t'_{3} \\

  (v r'_{31} - r'_{21}) x - (v r'_{32} - r'_{22}) y + (v r'_{33} - r'_{23}) z
  &=& t'_{2} - v t'_{3}  \\
  \end{array} \right.
\end{equation}

Because we are dealing with a ground point, $z = 0$ and we obtain an
invertible linear system:

:::puzzle
:::{.proposition name="Distance Calculation"}
\begin{equation}
  \left\{ \begin{array}{lll}
  (u r'_{31} - r'_{11}) x - (u r'_{32} - r'_{12}) y &=& t'_{1} - u t'_{3} \\
  (v r'_{31} - r'_{21}) x - (v r'_{32} - r'_{22}) y &=& t'_{2} - v t'_{3} \\
  \end{array} \right.
\end{equation}
:::
:::

This will determine the missing coordinates $x$ and $y$, which is
what we want.

### Generalization to Any Camera Model

So far our mathematical formulation assumes that the image formation is done via
perspective projection because we have assumed a pinhole camera model. In words,
each pixel $\mathbf{u} = \begin{bmatrix} u \\ v \end{bmatrix}$ of the image
$I(\mathbf{u})$ is hit by the 3D light ray vector $-\mathbf{x}$. With the
knowledge of the camera calibration matrix $\mathbf{K}$, we can calculate
$\mathbf{x}$ as

\begin{equation}
  \mathbf{x} = \mathbf{K}^{-1} \begin{bmatrix} \mathbf{u} \\ 1 \end{bmatrix},
\end{equation}

Notice the negative sign in the light ray vector because the light ray is in
front in the camera and passes through the camera aperture to hit the camera
film plane.

From a mathematical point of view, it may not be very practical to talk about
the light ray vector because of its negative sign. Instead we prefer to say that
any pixel $\mathbf{u}$ *backprojects* to the semi-line

\begin{equation}
  \{ \lambda \mathbf{x} : \lambda > 0 \}.
\end{equation}

In words any 3D scene point $\lambda \mathbf{x}$ of the semi-line are the only
points that

- are physically possible, and
- projects exactly to the pixel $\mathbf{u}$

because the true corresponding 3D scene point must (1) lie on this semi-line and
(2) be *in front of the camera*. The necessary condition $\lambda > 0$ ensures
that the 3D scene point is in front of the camera and is what we call the
*cheirality* constraint in the literature.

The perspective projection is actually a strong assumption nowadays in computer
vision because it assumes the camera model is the ideal pinhole camera. This is
not the case when we are dealing with more complicated cameras that have lens
distortions.

Also what happens if the image is instead formed from a fisheye camera with FOV
$> 180 \deg$? The fisheye camera is very attractive in robotics and in
surveillance applications because it can see the scene behind it to quite some
extent. The image formation is more complicated and the imaging via the pinhole
camera model is not valid anymore. That means the property of *similar
triangles* cannot be used anymore and this was key to deriving to estimate
distances.

However any well-formed and practical mathematical camera model should always
allow us to retrieve the corresponding direction of the 3D light ray
$\mathbf{x}$ for any pixel $\mathbf{u}$.

Before moving to the more general method for the distance estimation. Let us
expand the example of the fisheye camera model to fix ideas in our mind. In the
fisheye camera model, the direction $\mathbf{x}$ of the 3D light ray is
parameterized in spherical coordinates as

\begin{equation}
\mathbf{x} =
\begin{bmatrix}
  \cos \phi \cos \theta \\
  \sin \phi \cos \theta \\
  \sin \theta
\end{bmatrix}
\end{equation}

Then the image formation in the fisheye camera is summarized by the following formula
\begin{equation}
  r = 2 f \tan \frac{\theta}{2}
\end{equation}

where

- $r = \| \mathbf{u} - \mathbf{u}_0 \|_2$ and $\mathbf{u}_0 = \begin{bmatrix}
u_0 \\ v_0 \end{bmatrix}$ is the principal point,
- $u - u_0 = f \cos \phi$
- $v - v_0 = f \sin \phi$

A subtle yet important difference is that the light ray vector $\mathbf{x}$
does not satisfy the similar triangle property anymore.

However we still relate the direction of the light ray with the camera
extrinsics as follows

\begin{equation}
  \mathbf{R} \mathbf{x}_W + \mathbf{t} = \lambda \mathbf{x}
\end{equation}
