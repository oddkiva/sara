# Super Fast Separable Convolutions

Sounds boring?

I promise you this is a very interesting topic, maybe way more interesting than
you might think.

Despite its simple formula, it is actually very tricky to implement it in such a
way that we utilize the acceleration capabilities of the hardware to its
fullest, be it CPU, GPU or any chip specialized in accelerated computing.

Just like the discrete Fourier transform, I would like to emphasize how
important the convolution, generally speaking, is for digital information
processing. For example, the convolutional operation is already used in a large
spectrum of application ranging from low-level applications such as image
enhancement on camera devices to visual perception tasks relying on neural
networks.

For the semiconductor industry, this means reaching applications in digital
camera manufacturing, or the robotics, automotive, logistics industry.

Therefore we can understand why it has become such a critical business^[and by
this I mean billions (trillions?) of dollars ðŸ¤‘ðŸ’¸ðŸ’°ðŸ¤‘ðŸ’¸ðŸ’°ðŸ¤‘ðŸ’¸ðŸ’°] for any chip
design company like NVIDIA, AMD or ARM to provide such a platform, both software
and hardware, that can support the processing of such information either in real
time or at large scale. So it goes without saying that we also stand to gain a
lot in terms of technical know-how because of its ubiquity.

In this section, I am touching on the topic of *accelerated computing* on the
*software* side only.

"Alright, what's in it for me?", you might ask. In a nutshell, I can give you,
you as a developer, the tools to design new algorithms that take control of your
hardware capabilities or reimplement *algorithms that run much faster than
OpenCV* on your hardware with relatively little effort.^[without writing code
explicitly with Intel's or ARM's vector instruction API.]

Story time before we dive into the implementation of the Gaussian blur.

*If you have read me until here, I give you one bonus cookie: you can read this
section nonlinearly and skip to the subsections of interest.*


## Story Time

Once, I applied for a C++ technical leadership role for some company. I did a
behavioral interview with their CTO. He then told me who I would be interviewing
with in the next interviews. A few days later, he called me back for a quick
chat. He ended up telling me they would not move forward after examining
*Sara*'s image processing code. He did not really explain why. Nevertheless, the
most likely explanation I could come up with was that one of their senior
engineers disqualified me because all he seemed to care about was whether I
could understand and write code with CPU vector instructions.

The CTO profusedly apologized to me. He said politely that I certainly was
gifted but my code was not up to their standard. They must have deemed that my
image processing code was implemented too naively.

Indeed I remembered that during the behavioral interview, the CTO told me they
were doing lots of optimization involving CPU vector instructions in order to
process data as fast as possible. That it really was not that difficult and that
I could get up to speed fairly quickly, and blah blah blah... That was the only
reason for which, I believe, they disqualified me.

Having mostly an applied math background and being a self-taught programmer, it
did sound unfair and hypocritical to me. Hypocritical because, if it really was
easy, then why can you not learn it on the job? So, yes, it did make me feel
that you are never enough whatever you achieve in life.
Oh, so you're supposed to master every single damn thing of software engineering
when you start a job?
Going by the same logic, if David Lowe showcased his SIFT research code, he
would not qualify for the job either since I learnt from studying his code.

In hindsight I would not have been happy with the job anyways.

End of my angry rant and back to the topic: today how can we achieve that?
Nowadays, we have some answers with Halide, which allows us to do it very
elegantly.

## What is Halide?

In a technical jargon, Halide is an domain specific language embedded in C++.
The specific domain we are dealing is image processing. It is a language because
it compiles your optimized algorithm into assembly code.

With Halide, you can write an image processing filter and optimize for each
specific hardware and architecture. All of this elegangtly in a *very few lines
of code*. Then Halide will compile the C++ code into usable assembly code.

The main beauty with Halide is that you decouple:

1. the **algorithm**: in our case, the **separable convolution** and,
2. the **schedule**: which leverages the different **kinds of parallelism** that
   the hardware offers,

so that our implementation is as fast as the baseline implementation. In our
case, the baseline implementation is OpenCV's Gaussian blur.

Halide abstracts and unifies the **common programming patterns** that arise from
CPU to GPU programming in CUDA, OpenCL, Metal, Vulkan. Besides, an image
processing algorithm can often expressed as a sequence of CUDA kernels, you
don't need to write any nested loops.

Halide abstracts different kinds of parallelism and supports a wide range of
platforms. While we don't have to know the C API to invoke vector instructions,
we still need to know those common programming patterns that involves CPU vector
instructions.

When I began learning Halide, I knew very little about optimal schedules.
Skimming through publications and the presentations by Halide authors was the
only way for me to really learn. And of course practise, practise, practise. So
all in all, in my experience, the entry barrier is still very high for the
average programmer to identify which schedules work best and those that don't.

### Halide vs Explicit CPU intrisic code

Naysayers will argue: "But Halide is too high level and does too much magic!"

Certainly, optimizing algorithms by explicitly writing CPU vector instructions
can be done. We would have to pay a very costly engineering time, however. If I
were the engineering head of a small business, this would be hard to justify.
And time is money.

Nowadays, who would be ready spend time on optimizing for different CPU
platforms: *x86-64*, *ARM*, *RISC-V* and learn their respective C API to invoke
SIMD instructions? On top of that, the resulting code would be much lengthier
and harder to debug and maintain than using a unified language that allows you
to write these in a very few lines of codes.

Unless this is your full-time job or it's something you really want to learn...
I wouldn't spend time into this. Either way, Halide has done an tremendous job
in making CPU vector instructions *platform-agnostic*. And it will help you
learn the vector instructions API provided much faster.

Let's conclude this paragraph with a few words regarding the GPU acceleration
with Halide. On the GPU side, Halide does not seem mature yet. For one thing,
the documentation is still lacking regarding the memory model at this time of
writing. You will be much better off writing code in CUDA, Vulkan, or OpenCL to
fully control your GPU.


## A Naive Implementation of the Gaussian Blur in C++

Right, let's rewind back in time and imagine myself more than 15 years ago.
Freshly graduated with a Master's degree looking to showcase some work
portfolio. I am a bit aware of CPU registers but I can't see how they would fit
in the grand scheme of things. I've heard of vector instructions but never
understood what they were about. As an avid Internet reader, I have been sold
the big lie: don't prematurely optimize and just let the compiler do its job.
Just make things work.

I did learn in class that the Gaussian filter is separable: Don't write the 2D
convolution naively, exploit its separable property. Icing on the cake, I have
discovered multicore parallelism via OpenMP. I am proudly exhibiting a naive
implementation in C++ in my *Sara* repo, something along the lines below.

### The x-convolution

Below is the first step, that is the x-convolution.
```{Rcpp}
#include <algorithm>

auto conv_x(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;

      // Calculate the convolved value.
      for (auto k = 0; k < ksz; ++k)
      {
        // Check the boundary conditions.
        const auto xk = std::clamp(x - r + k, 0, w - 1);

        // Accumulate.
        const auto xy = y * w + xk;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

### The y-convolution

Then we perform the same dance for the y-convolution in a similar fashion.

```{Rcpp}
#include <algorithm>

auto conv_y(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;

      // Calculate the convolved value.
      for (auto k = 0; k < ksz; ++k)
      {
        // Check the boundary conditions.
        const auto yk = std::clamp(y - r + k, 0, h - 1);

        // Accumulate.
        const auto xy = yk * w + x;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

I diligently write unit tests, validate on synthetic tests, check the boundary
conditions and try it on a real image with a Gaussian kernel. I am happy it
works reasonably fast when compiling in Release mode on Visual Studio. Job done!
I am proudly showcasing my code on GitHub. Never complained about it as I never
bothered about real-time considerations. Instead I would port the code in CUDA
to make the algorithm run faster.

### Issues in the C++ code

Let's enumerate some issues and ideas that we will address later on.

1. The only parallelism we are using is the **multi-threading**.

   Each row is processed in parallel but can we do better than parallel row
   processing?

2. It is not clear how the **CPU vector instructions** can be applied in the
   current implementation.
   - For one thing, the boundary check via clamping does not easily allow the
     compiler to vectorize the C++ code.
   - There is actually a better way to exploit the CPU vector instructions for
     the convolutional operation and the C++ code does not do this way.

3. **Data locality** is a very important aspect, which the parallel row
   processing may not easily allow to do so.

   Just like in CUDA programming, (1) splitting an image into smaller image
   tiles and (2) processing each of these image tiles independently on CPU is
   much more efficient in practice.

   By splitting an image into smaller image tiles and processing them
   individually, we solve the data locality problem. Indeed all the pixel data
   in this image tile can be copied and fit in the small memory cache, which is
   much faster than the RAM.

   Doing so enables the CPU fetch the source image data from the RAM only once,
   whereas unoptimized implementations would force the CPU to fetch the image
   data from the RAM much more often.

Let's note that processing image tiles in parallel is a pattern that keeps
recurring in optimized linear algebra routines (OpenBLAS etc.), specifically the
matrix-matrix multiplication *gemm* operation, which is also one fundamental
low-level operation in deep learning.

There are probably more issues I have overlooked but I pointed out the most
problematic ones. Certainly it wasn't the fault of my younger self for not
knowing any better. Let us now resolve these issues in Halide and... educate my
younger self.


## A Very Fast Implementation in Halide

As we said it earlier, Halide splits an algorithm into two parts:

1. the implementation and
2. the schedule for any algorithm.

We will now detail each part. This section will divided in 3 parts.

1. First we implement the separable convolution in Halide.
2. Then we explain the different kinds of parallelisms that the CPU offers.
3. Finally we explain the schedule part of the algorithm in Halide and we will
   explore a few different schedules.

### The Algorithm

First, let us write the implementation that exploits the separability of the
Gaussian filter.

```{cpp}
#include <Halide.h>

auto x = Halide::Var{"x"};
auto y = Halide::Var{"y"};

auto input = Halide::Func{"input"};
auto kernel = Halide::Func{"kernel"};

static constexpr auto sigma = 3.f;
static constexpr auto truncation_factor = 4.f;
static constexpr auto ksz = static_cast<int>(2 * sigma * truncation_factor);
static constexpr auto kr = ksz / 2;

// N.B.: we just need to define something for the input... but just think of it
// as an image.
input(x, y) = x + y;

// The 1D Gaussian filter.
kernel(x) = Halide::exp(-x * x / (0.5f * sigma * sigma));

auto conv_x = Halide::Func{"conv_x"};
auto conv_y = Halide::Func{"conv_y"};

auto k = Halide::RDom(0, ksz, "k");

// The algorithm
conv_x(x, y) = Halide::sum(input(x + k - kr, y) * kernel(k));
conv_y(x, y) = Halide::sum(conv_x(x, y + k - kr / 2) * kernel(k));
```

### Two Types of Parallelisms on CPUs

There are two types of parallelisms on the CPU which we can exploit altogether.

1. *Multicore processing*:

   This is straightforward to understand. A CPU can be thought as a factory of
   workers, each one of them being called a CPU core.

   The multicore processing becomes most effective when each CPU core is being
   kept as busy as possible and the CPU cores don't need to communicate with
   each other via data synchronization.

   OpenMP is one implementation of multicore processing among others to
   parallelise our image filter.

2. *Vector instructions*:

   Until I implemented filters with Halide, I did not understand what CPU vector
   instructions were really about.

   Mainly, a CPU vector instruction enables a CPU core to perform arithmetic
   operations on small vectors in a **single** CPU cycle.
   That means that additions, subtractions, dot products on 4D float vectors can
   be executed in a single CPU cycle instead of 4 CPU cycles (7 CPU cycles for
   the dot product). **(PLEASE RESEARCH THIS MORE: this is not true although the
   idea is there...)**

   The performance is very significant on a large scale as we can observe
   additional 4x speed up or more.

Nowadays, an Intel CPU that supports AVX-512 vector instructions can perform
operations on 16D vectors of 32-bit floating point data in a single CPU
instruction.

By combining parallelisms 1 and 2 on an 12-core Intel CPU with AVX-512
instructions, an optimized algorithm could in principle be sped up by a factor
of

\begin{equation}
  \#\{\text{CPUs}\} \times \text{SIMD max dim} = 12 \times 16 = 192
\end{equation}

on single-precision floating point data.

While we should instead use *Amdahl's law* instead of this naive formula, it is
not so simple to apply it and we will be content to just benchmark and measure
the performance gain.

Likewise, optimized linear algebra routines like OpenBLAS makes extensive use of
the two types parallelism on CPU platforms.

For more accurate and more comprehensive information, I will simply encourage
you to do your own research on ARM, RISC-V CPUs, and share what you have learnt.

Let's move on to the algorithmic schedule, which is the most difficult part of
the implementation. We will explore 3 schedules with Halide.

### Schedule 1: Naive Strategy

The obvious strategy is to start from the idea of separable convolution and
vectorize the convolution wherever possible.

We parallelize the computation of image rows. We apply Halide's magic invocation
to vectorize the convolution without trying to really understand why it works.

```{cpp}
// Precompute the kernel all at once.
//
// This is irrelevant in the schedule but necessary when you want to understand
// the assembly code generated by Halide.
kernel.compute_root();

// First, compute the x-convolution in a separate memory buffer and use CPU
// vector instructions.
conv_x
    .compute_root()
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;

// Second, compute the y-convolution as a second step.
conv_y
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf) ;
```

This schedule is a nice improvement over the naive C++ implementation as we
exploit the CPU data vectorization. Yet OpenCV's implementation is still better
than this schedule by a factor 2.

Later we will examine how the CPU vectorization is implemented by inspecting a
better schedule. So let's move on to a better schedule: schedule #2.

### Schedule 2: From a Publication

The optimal schedule is really not obvious in CPU, at least for me. After
digging in the publications, I found this schedule in one of Halide
publications:

```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

auto tile_index = Halide::Var{"t"};

conv_y.tile(x, y, xo, yo, xi, yi, 64, 64,
            Halide::TailStrategy::GuardWithIf)
    .fuse(xo, yo, tile_index)
    .parallel(tile_index);
    .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
```

This literally crushes OpenCV's implementation but I didn't understand why at
that time. Still we do see the main optimization ideas in this schedule which is
to split the image into smaller image tiles and processing tiles so that data
locality is utilized. Truly the unclear part is **how Halide applies the vector
instruction within each image tile** and the publication did not explain it in
details.

As argued in a presentation by Halide authors, without Halide, the optimal
schedule is definitely not obvious and emphasized that at least 3 months of work
was necessary to find out how to best utilize CPU SIMD instructions.

Specifically this schedule the final convolved image into square tiles of 64 by
64 pixels. The set of input tiles can be seen as an input batch of many smaller
images and processed in parallel. The output image is obtained by reassembling
the batch of output image tiles.

We will detail later how the vectorization of the convolution is done.

### Schedule 3: An Improved Version by Myself

Because each output image tile is independent of each other, we can calculate
their x-convolution and y-convolution that can fit in the memory cache. For each
tile we can fit the x-convolution in the CPU cache and it improves data locality
in the memory cache. Then we exploit the CPU vector instructions to calculate
convolution in batch.


```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

auto tile = Halide::Var{"tile"};

// Precalculate the kernel.
// We want this to avoid cluttering Halide's compiled statement later on.
kernel.compute_root();

// The schedule
conv_y
    .tile(x, y, xo, yo, xi, yi, 64, 64)
    .fuse(xo, yo, tile)
    .parallel(tile)
    .vectorize(xi, 32, Halide::TailStrategy::GuardWithIf)
    ;
conv_x
    .compute_at(conv_y, xi)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;
```

### Second-Guessing What Halide Does

There is a lot to unpack here. Let's try to break it down bit by bit the
schedule below:

```{cpp}
  conv_y.tile(x, y, xo, yo, xi, yi, 64, 64,
              Halide::TailStrategy::GuardWithIf)
      .fuse(xo, yo, tile_index)
      .parallel(tile_index);
      .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
      ;
```

According to how I understand it:
```{cpp}
#pragma omp parallel for
for (auto tile_index = 0; tile_index < T; ++T)
{
  // Process the tile (xo, yo).
  const auto yo = tile_index / T;
  const auto xo = tile_index % T;

  for (auto yi = 0; yi < 64; ++yi)
  {
    for (auto xi = 0; xi < 64; ++yi)
    {
      const auto xi_begin = xo * 64 + xi;
      const auto xi_end = (xo + 1) * 64 + xi;

      // Recompose x and y.
      const auto y = yo * 64 + yi;
      const auto x = xo * 64 + xi;


      // compute_at(...) allocates memory on the stack as explained in:
      // https://halide-lang.org/docs/class_halide_1_1_func.html#a800cbcc3ca5e3d3fa1707f6e1990ec83.
      //
      // I understand that the storage **has** to be 2D, if we want to calculate
      // conv_y to ensure maximum data locality.
      float conv_x[ksz][ksz];

      // Trivially vectorizable.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] = 0.f;

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] += in[y][x + k] * kernel[k];

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++ksz)
        conv[y][x] = 0.f;

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++ksz)
        conv_y[y][x] += conv_x[yi+k][xi] * kernel[k];
    }
  }
}
```

It is not yet clear to me how the convolutions are vectorized in the variable
`xi`. This is the goal of the next paragraph.

### Understanding with Halide Compiled Statement

The second-guessing turns out to be not that bad but we really need to
understand what Halide does for us. How is the CPU vectorization done?

To get a more definite answer, let's actually inspect the assembly code
generated by Halide.

```{cpp}
conv_y.compile_to_stmt(
  "separable_conv_2d.stmt.html",
  {},
  Halide::HTML
);
```

Halide generates a nicely illustrated HTML document. Still The generated code is
overwhelming as there is a lot to unpack. How are we going to learn how to read a bit of assembly code?

To digest the algorithmic flow, I started by commenting out the different
parallelisms in the schedule:

1. the multicore parallelism
   ```{cpp}
   conv_y.
     ...
     // .fuse(xo, yo)
     // .parallel(tile)`
   ```
2. the vectorization parallelism
   ```{cpp}
   conv_y.
     ...
     // vectorize(xi, 32, GuardWithIf);

   conv_x.
     ...
     // vectorize(xi, 32, GuardWithIf);
   ```

This will help us to break down the algorithmic roadmap and understand how the
HTML visually maps out the **three-way** correspondence between:

1. each part of the pseudo-code,
2. each diagram block and
3. each part of the assembly code.

After a bit of inspection, by reinstating progressively all the different
parallelisms, we start to understand how the parallel patterns are implemented
on assembly code.

This is how it looks like.
<iframe src="./random/separable_conv_2d.stmt.html" height="405" width="720"
style="border: 1px solid #464646;" allowfullscreen>
</iframe>

It is not practical to view the embedded HTML page, so you can click on this
link <a href="./random/separable_conv_2d.stmt.html">link</a> to fully explore
the compiled statement as a full page:


### Vectorizing the Convolution

This paragraph is the most technical part of the writing and it will require a
bit of time to unwrap the algorithm. I would recommend to draw things on the
paper.

Let us examine the vectorization of the x-convolution as the same rationale
applies to the y-convolution.

An important technical assumption in the following is that the image data is
stored in a row-major format, that is the image is stored row by row in the
memory.

Upon inspection of the compiled statement, it turns out that the convolution
operation is implemented by batch where we calculate 4, 8 or 16 convolved values
at the same time by repeating invoking the vectorized fused multiply-add
`fmla.4s` operation in the following manner.

Because an image $x$ is split into a batch of small image tiles, we can
reimagine the image as a 3D tensor $\mathbf{x}$. Each element of the tensor
$\mathbf{x}[t, i, j]$ being read as the pixel value at coordinates $(j, i)$ of
image tile $t$.

For now, let's simplify by omitting the index $t$ and only consider one image
tile so that we can alleviate the notation and can keep reasoning in 2D. Instead
of denoting a source image tile and a destination image tile by
$\mathbf{x}[t, :, :]$ and $\mathbf{y}[t, :, :]$ respectively, we simplify denote
them by $\mathbf{x}$ and $\mathbf{y}$

Mathematically speaking, to calculate each convolved value individually
\begin{equation}
\mathbf{y}[i, j] = \sum_k g[k] \times \mathbf{x}[i, j{+}k]
\end{equation}

In pseudo-code, we would apply the **unvectorized** FMA operation as follows:

>
$\mathbf{y}[i, j] \leftarrow 0$
>
>
> For $k = -K$ to $K$:
>
$\qquad$ $\mathbf{y}[i, j]
\leftarrow
\mathbf{y}[i, j] + g[k] \times \mathbf{x}[i, j{+}k]$
$\quad$ (FMA operation)

Now instead of calculating each convolved value individually, we repeatedly
apply the vectorized FMA operation on the first 4 pixels on the same image row
(or more if the CPU supports more) in each image tile.

Mathematically, this means that we consider the following 4D row-vector:

\begin{equation}
\mathbf{y}[i, 0{:}4] = \sum_k g[k] \times \mathbf{x}[i, 0{+}k{:}4{+}k]
\end{equation}

In pseudo-code, it appears we can apply the **vectorized** FMA operation on this
4D row-vector:

>
$\begin{bmatrix}
\mathbf{y}[i, 0] \\
\mathbf{y}[i, 1] \\
\mathbf{y}[i, 2] \\
\mathbf{y}[i, 3]
\end{bmatrix}
\leftarrow
\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix}$ $\quad$ (assignment is parallel)
>
>
> For $k = -K$ to $K$:
>
$\qquad$ $\begin{bmatrix}
\mathbf{y}[i, 0] \\
\mathbf{y}[i, 1] \\
\mathbf{y}[i, 2] \\
\mathbf{y}[i, 3]
\end{bmatrix}
\leftarrow
\qquad$ $\begin{bmatrix}
\mathbf{y}[i, 0] \\
\mathbf{y}[i, 1] \\
\mathbf{y}[i, 2] \\
\mathbf{y}[i, 3]
\end{bmatrix}
+
\begin{bmatrix}
g[k] \\
g[k] \\
g[k] \\
g[k]
\end{bmatrix}
\odot
\begin{bmatrix}
\mathbf{x}[i, 0{+}k] \\
\mathbf{x}[i, 1{+}k] \\
\mathbf{x}[i, 2{+}k] \\
\mathbf{x}[i, 3{+}k]
\end{bmatrix}$
>
> $\qquad$ (FMA is a parallel operation)


Then we move onto the next 4D row-vector in the same image row and applies the same
algorithm above.

>
$\mathbf{y}[i, 4{:}8] = \sum_k g[k] \times \mathbf{x}[i, 4{+}k{:}8{+}k]$
>
$\mathbf{y}[i, 8{:}12] = \sum_k g[k] \times \mathbf{x}[i, 8{+}k{:}12{+}k]$
>
> ...

Then we move onto the next row of the image tile and so on.


There is a crucial technical detail we need to realize. Because the image data
is in a row-major storage format, we only re-read the image tile data in the
cache, without ever needing to copy the image data with the appropriate order in
the CPU register, everything is contiguous during the process.

Equivalently in NumPy code, this time where we view an image as a 3D tensor:

```{python}
import numpy as np

def convolve_vectorized(conv_x, input, kernel, t, i, j):
    # Trivially vectorized.
    conv_x[t, i, j:j+4] = 0

    # Repeat the fused multiply-add operation as follows.
    #
    # I leave the boundary checks to you as an exercise...
    ksz = kernel.shape[0]
    for k in range(ksz):
        conv_x[t, i, j:j+4] = \
            conv_x[t, i, j:j+4] + \
            input[t, i, j+k:j+4+k] * kernel[k]
```

To apply the same rationale for the y-convolution, there is one more technical
detail to be aware.

We need to store the transposed x-convolution data (as in matrix transpose) to
ensure that the data contiguity of image columns in memory. That means that the
x-convolved image (and thus its tiles) must be stored in a **column-major**
format. Then the final separable convolved image must be stored back in
**row-major** format.

References:

- https://diveintosystems.org/book/C9-ARM64/common.html
- https://developer.arm.com/documentation/
