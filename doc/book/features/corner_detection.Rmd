# Corners and Junctions

Detecting corners can be done in many different ways. One robust way to do so is
to analyze of the local distribution of image gradients at each pixel of the
image. From there, we can identify the dominant orientations of the gradients.

## Histogram of Gradients as Insight

:::fyi
OK, I agree this is not the real intuition that lectures or publications start
with but keep on reading what I have to say.

I don't see why it can't be be a valid intuition.
:::

We can calculate one histogram of local gradients per pixel and then localize
the dominant gradient orientations which are localized at the histogram peaks.

Intuitively we expect to identify interpretable local image features:

- an edge if we count only $1$ dominant gradient orientation.
- a square-like corner if we count $2$ dominant peaks.
- a T-junction if we count $3$ dominant peaks.
- a chessboard X-corner if we count $4$ dominant peaks.
- and so on.

While, by design, the histogram cannot localize very precisely the corners, I
think it is important to be reminded that the histogram of gradients is still a
powerful tool in practice. I have found it to be mostly useful in filtering
unwanted corners. [@Lowe:2004:ijcv] describes a robust way to identify the
dominant gradient orientations.

## Small Shifts Inducing Large Intensity Changes

The histogram of gradients does not tell us precisely whether a pixel is located
a corner or a junction. It can only tell whether or not this pixel is close to a
corner and that depends on the size of the local window we choose when we build
the histogram of local gradients.

To localize corners accurately, [@Moravec:1980:phd]'s starting point is to look
for pixels $\mathbf{x}$ where there exists a large intensity difference

\begin{equation}
  \left( I(\mathbf{x} + \mathbf{h}) - I(\mathbf{x}) \right)^2
\end{equation}

for a **small and specific** set of **nonzero** shifts $\mathbf{h}$.^[I won't go
into much details and I will let you find out more by looking into
[@Moravec:1980:phd]'s thesis. [@HarrisS:1988:alvey]'s formulation is simple
enough to get the whole picture about corner detection.]

Now, instead of considering an *ad-hoc* small set of nonzero shifts,
[@HarrisS:1988:alvey] do better by considering the sum of **weighted** squares of
intensity differences for any small shift $\mathbf{h}$.

\begin{equation}
  \sum_\mathbf{h} w(\mathbf{h})
  \left( I(\mathbf{x} + \mathbf{h}) - I(\mathbf{x}) \right)^2
\end{equation}

Then surely this quantity must respond maximally at a corner location? Yes it is
the case in practise.

Let's observe the first-order Taylor expansion

\begin{equation}
  I(\mathbf{x} + \mathbf{h}) =
  I(\mathbf{x}) + \mathbf{h}^T \nabla I(\mathbf{x}) + o(\|\mathbf{h}\|)
\end{equation}

then it follows that the reweighted sum of squares is approximated as

\begin{equation}
  \sum_\mathbf{h} w(\mathbf{h})
  \ \mathbf{h}^T
  \left(
    \nabla I(\mathbf{x}) \nabla I(\mathbf{x})^T
  \right)
  \mathbf{h},
\end{equation}

where the nabla operator $\nabla$ expresses the gradient as a column-vector
\begin{equation}
  \nabla I = \begin{bmatrix}
    \frac{\partial I}{\partial x} \\
    \frac{\partial I}{\partial y}
  \end{bmatrix}.
\end{equation}

As pointed by [@HarrisS:1988:alvey], we recognize the covariance matrix of local
gradients, which the corner detection will be based on. This is the so-called
*structure tensor*. Another innovation by [@HarrisS:1988:alvey] is to also
derive a **cornerness** score function from it. This function will score highest
to corners and even more so at their precise location. The cornerness function
solves the problem of corner localization.

Let us talk more in details about the structure tensor first and we will expand
a bit more on the cornerness later on.

## Structure Tensor

As we said it in the previous section, the structure tensor is a covariance
matrix of local gradients.

Let us define the structure tensor in the discrete domain first. Then we
redefine it in the continuous domain. The continuous definition is useful to
provide insights for a better and more efficient implementation in the discrete
domain.

### Discrete Domain

Formally we consider all image gradients $\nabla I(\mathbf{x} + \mathbf{h})$
within the image patch centered in $\mathbf{x}$ with radius $r$, that is, the
set of pixels $\mathbf{x} + \mathbf{h}$ such that $|| \mathbf{h} || < r$.
The gradients are reweighted by a weight function $w(\mathbf{h})$ which gives
them more importance if they are closer to $\mathbf{x}$. Classically, the
Gaussian kernel is used.

In the discrete domain, the structure tensor is written as

\begin{equation}
  \mathbf{M}(\mathbf{x}) =
  \displaystyle \sum_\mathbf{h} w(\mathbf{h})
  \begin{bmatrix}
    \left( \frac{\partial I
    (\mathbf{x} + \mathbf{h})}{\partial x} \right)^2 &&
    %
    \frac{\partial I(\mathbf{x} + \mathbf{h})}
         {\partial x}
    \frac{\partial I(\mathbf{x} + \mathbf{h})}
         {\partial y} \\
    %
    \frac{\partial I(\mathbf{x} + \mathbf{h})}
         {\partial x}
    \frac{\partial I(\mathbf{x} + \mathbf{h})}
         {\partial y} &&
    %
    \left( \frac{\partial I
    (\mathbf{x} + \mathbf{h})}{\partial y} \right)^2
  \end{bmatrix}
\end{equation}

### Continuous Domain

Since we mentioned the Gaussian kernel, which is **symmetric**, we can
generalize the definition in the continous domain as a convolution with any
distribution $w$.

\begin{equation}
  \mathbf{M}(\mathbf{x}) =
  \displaystyle \int w(\mathbf{h})
  \begin{bmatrix}
    \left( \frac{\partial I (\mathbf{x} -
    \mathbf{h})}{\partial x} \right)^2 &&
    %
    \frac{\partial I(\mathbf{x} - \mathbf{h})}
         {\partial x}
    \frac{\partial I(\mathbf{x} - \mathbf{h})}
         {\partial y} \\
    %
    \frac{\partial I(\mathbf{x} - \mathbf{h})}
         {\partial x}
    \frac{\partial I(\mathbf{x} - \mathbf{h})}
         {\partial y} &&
    %
    \left( \frac{\partial I (\mathbf{x} -
    \mathbf{h})}{\partial y} \right)^2 &&
  \end{bmatrix}
  d\mathbf{h}
\end{equation}

Now as a side note, in the sense of Lebesgue's integration, this definition
coincides with the discrete definition using a discrete measure.

We can re-express this more tersely, in functional notation with the $*$ symbol
to denote a convolution operation

\begin{equation}
  \mathbf{M} = w * \left( \nabla I\ \nabla I^T \right)
\end{equation}

### Interpretation

Because the structure tensor is a covariance matrix, the structure
tensor should capture in principle only two principal directions of the image
gradients. The gradient directions are expected to be quasi-orthogonal.

This is perfect for a square-like corner where only two gradient orientations
should dominate. The structure tensor can also localize an edgel, only one
gradient orientation should dominate.

Indeed, the extraction of the eigenvectors will give us the two principal
directions of the gradients, and its associated eigenvalues $\lambda_1,
\lambda_2$ with $\lambda_1 > \lambda_2$  quantifies the "frequency" of these two
principal directions.

- If the local patch is **a uniform region**, the gradients are very small, the
  covariance matrix should be close to zero, i.e., $\lambda_i \approx 0$.
- If the local patch is on **an edge**, the covariance matrix should correspond
  to a very elongated ellipse geometrically. The ratio between the lowest and
  highest eigenvalues should be very small, i.e., each $\lambda_2 \ll \lambda_1$
- If the local patch is on **a corner**, then covariance matrix should
  correspond to a large circle. The ratio between the lowest and highest
  eigenvalues should be close should be close to $1$, i.e.,
  $\frac{\lambda_1}{\lambda_2} \approx 1$

In principle, the structure tensor should detect only these types of features,
corners or edgels. In practise however, it will respond strongly to junctions as
well. This is one problem of the structure tensor because this relies on the
assumption that edges are orthogonal...

### Other Practical Considerations

In practice the image $I$ is first convolved with a Gaussian kernel $g$ of
standard deviation $\sigma_D$. This is to reduce the aliasing and image noise.

\begin{equation}
  \mathbf{M} = g_{\sigma_I} * \left( \nabla I_{\sigma_D} * \ \nabla I_{\sigma_D}^T \right)
\end{equation}
where the convolved image with a gaussian kernel is denoted as
\begin{equation}
  I_{\sigma_D} = g_{\sigma_D} * I
\end{equation}

We need to mention another important consideration related to the **scale-space
theory**.
Assuming that the scale of the original image $I$ is exactly $1\ \textrm{pixel}$
($\textrm{px}$)^[Since the camera cannot capture details less than 1 pixel on
images in principle.], then the scale of the convolved image $I_{\sigma_D} =
g_{\sigma_D} * I$ is

\begin{equation}
  t = \sqrt{1 + \sigma_D^2}\ \textrm{px}
\end{equation}

This has an important consequence in practice.

1. Image details within a window of $t$
pixels are lost because of the Gaussian blur -- in principle.

2. The integration scale $\sigma_I$ in the structure tensor means that we are
considering local patches with radius $t \sigma_I = \sqrt{1 + \sigma_D^2}
\ \sigma_I$ to decide whether we see a
corner or not. This means that the best corners are separated by at least
$t \sigma_I$ pixels.

### Remarks About Efficient Implementations

In practise, the convenient form that really allows for an optimized
implementation is the following one

\begin{equation}
  \mathbf{M} = \begin{bmatrix}
    w * \left( \frac{\partial I}{\partial x} \right)^2 &&
    %
    w *
    \left( \frac{\partial I}{\partial x} \frac{\partial I} {\partial y} \right) \\
    %
    w *
    \left( \frac{\partial I}{\partial x} \frac{\partial I} {\partial y} \right) &&
    %
    w *
    \left( \frac{\partial I}{\partial y} \right)^2
  \end{bmatrix}
\end{equation}

Because the structure tensor matrix is symmetric, efficient implementations
calculate only the upper-triangular terms. They perform the following operations
in the following order:

1. The derivative maps $\frac{\partial I}{\partial x}$ and
   $\frac{\partial I}{\partial y}$.
2. The product maps
   $\left(\frac{\partial I}{\partial x}\right)^2$,
   $\left(\frac{\partial I}{\partial y}\right)^2$ and
   $\frac{\partial I}{\partial x} \frac{\partial I}{\partial y}$
3. The convolved maps
   $w * \left( \frac{\partial I}{\partial x} \right)^2$,
   $w * \left( \frac{\partial I}{\partial y} \right)^2$ and
   $w * \frac{\partial I}{\partial x} \frac{\partial I}{\partial y}$

On any platforms, the trickiest operation to optimize is the convolution. This
is especially true on CPU architectures, where it is notoriously hard to find
the optimal scheduling as pointed out in [@RaganKelleyBAPDA:2013:pldi].


## Cornerness Function

**TODO: write more explanations.**

In a nutshell, the cornerness function is the scalar function (in functional
notation)

\begin{equation}
  C = \det(\mathbf{M}) - \kappa \mathrm{trace}(\mathbf{M})
\end{equation}

In practice, I have found that $\kappa = 0.04$ works very well in my experiments
and found little reason to change it. [@HarrisS:1988:alvey] suggests a range
with this value being the this lower bound.

## Förstner's Junction Operator

Harris's detector localizes corners with an accuracy of $1$ pixel at best. To
get a sub-pixel accuracy, we can refine the corner location using Förstner
criterion.

Förstner's idea comes from the observation that a good corner should be lying on
the two dominant edges, if we look at a small patch centered at the corner
location $\mathbf{x}$.

So we just said that $\mathbf{x}$ must be an edgel. Let's discuss two cases.
- If a neighboring pixel $\mathbf{x}'$ is also an edgel, we expect
  that the direction vector $\mathbf{h}$ is a good approximation of the
  (quasi-straight) edge curve and should be normal to the image gradient $\nabla
  I(\mathbf{x}')$.

  That means that the dot product between that the edge direction and the image
  gradient should be very small.
- If a neighboring pixel is not an edgel, its image gradients should be close to
  zero and it will not bring any information.

Now, since the corner location $\mathbf{x}$ is quantized, we can refine its
location by minimizing the sum of square dot products over all points in the
patches. The sum of squares can be reweighted by say a Gaussian kernel $w(.)$ to
give more importance to pixels $\mathbf{x}'$ closer to the approximate corner
location $\mathbf{x}$.

And Förstner's criterion becomes

\begin{equation}
  \displaystyle \min_\mathbf{x}
  \sum_\mathbf{x'} w\left(\mathbf{x} - \mathbf{x}'\right)
  \left\{
    \left(\mathbf{x} - \mathbf{x}' \right)^T \nabla I(\mathbf{x}')
  \right\}^2
\end{equation}

where the neighboring pixels $\mathbf{x}'$ are in a small image patch
$\|\mathbf{x} - \mathbf{x}'\| < r$.

In fact, we can calculate the sum of square dot-products at every single pixel
of the image as every corner would correspond to a local minimum of this such
criterion, But Harris's corner detection method is computationally more
efficient.

:::note
Until now, I have talked about corners but we can easily convince ourselves that
that Förstner's criterion works for any complex junctions, say an x-corner,
a T-junction.
:::


## Spiral Model

I learnt from [@ForstnerDS:2009:iccv] that the spiral model is an extension of
the structure tensor. This time, the structure is also parameterized with a 2D
rotation matrix parameterized by an angle $\alpha$ in the structure tensor.

\begin{equation}
  \mathbf{M}(\alpha) = w *
  \mathbf{R}_\alpha \nabla I \nabla I^T \mathbf{R}_\alpha^T
\end{equation}

