# Super Fast Separable Convolutions

Sounds boring? I promise you this is going to be way more interesting than you
might think. While this is a common operation in low-level operation, there is
quite a few interesting things to learn how to leverage the hardware to
accelerate computing.

Story time before we dive into the implementation of the Gaussian blur.

## Story Time

Once, I applied for a C++ technical leadership role for some company. I did a
behavioral interview with their CTO. He then told me who I would be interviewing
with in the next interviews. A few days later, he called me back for a quick
chat. He ended up telling me they would not move forward after examining sara's
image processing code. Without really explaining why, the most likely reason I
could guess was that one of their senior engineers disqualified me because all
he cared about was whether I could understand and write code with CPU vector
instructions.

The CTO profusedly apologized to me. He said politely that I certainly was
gifted but my code was not up to their standard. They must have deemed that my
image processing code was implemented too naively.

During the behavioral interview, the CTO told me they were doing lots of
optimization involving CPU vector intrinsics in order to calculate data as fast
as possible. That it really was not that difficult and that I could get up to
speed fairly quickly. That was the main reason I believe they disqualified me.

Having mostly an applied math background, it did sound unfair and hypocritical
to me. Hypocritical because, if it really was easy, then why can you not learn
on the job?
So it did make me feel that you are never enough whatever you achieve in life.
Oh, so you're supposed to master every single damn thing of software engineering
from high level to low level when you start a job?
Going by the same logic, if David Lowe showcased his SIFT research code, he
would not qualify for the job either since I learnt from studying his code.

In hindsight I would not have been happy with the job anyways. End of the rant
and back to the topic: today how can we achieve that? Nowadays, we have some
answers with Halide. And we can do it very elegantly without coding in assembly
code directly.

## What is Halide?

In a technical jargon, Halide is an domain specific language embedded in C++.
The specific domain we are dealing is image processing. It is a real language
because it compile your optimized algorithm into assembly code.

With Halide, you can write an image processing filter and optimize for each
specific hardware and architecture. All of this elegangtly in a *very few lines
of code*. Then Halide will compile the C++ code into usable assembly code.

The main beauty with Halide is that you decouple:

1. the algorithm: in our case, the separable convolution and,
2. the scheduling strategy: which exploits the different kinds of
   parallelism that the hardware offers,

so that our implementation is as fast as the baseline implementation. In our
case, the baseline implementation is OpenCV's Gaussian blur.

Halide can check and provide guarantees that your algorithm remains correct for
the schedule you are implementing.

Halide is trying to unify the common programming patterns from CPU to GPU
programming in CUDA, OpenCL, Metal, Vulkan. Similarly to CUDA, an image
processing algorithm can often expressed as a succesion of CUDA kernels.
Therfore you don't need to write any nested loops.

Halide abstracts the different kind of parallelisms for you and supports a wide
range of platforms. You don't have to be an expert in CPU vector intrinsics, but
you do need to know some scheduling strategies to best optimize the
convolutional operation.
When I began learning Halide, I knew very little about optimal schedules.
Skimming through publications and the presentations by Halide authors was the
only way for me to really learn. And of course practise, practise, practise. So
all in all, in my experience, the entry barrier is still very high for the
average programmer to identify which schedule patterns work best and those that
don't.

### Halide vs Explicit CPU intrisic code

Naysayers be like: "but Halide is too high level and does too much magic!"

Certainly! Optimizing algorithms by writing explicitly CPU intrinsic
instructions can be done. But you would have to pay a very costly engineering
price.

You would have to optimize for different CPU platforms: x86-64, ARM, RISC-V and
learn about their C API to utilize SIMD instructions. The resulting code would
be much harder to maintain than using a unified language that allows you to
write these in a very few lines of codes.

Unless this is your full-time job or it's something you really want to learn,
personally I don't want to spend time into this. Halide has done an excellent
job in abstracting this at the least on the CPU side. So know who you are and
decide what you want to do.

Let's conclude this paragraph with a few words regarding the GPU acceleration
with Halide. On the GPU side, Halide is indeed not yet mature. For one thing,
the documentation is still lacking regarding the memory model at this time of
writing. You will be much better off writing code in CUDA, Vulkan, or OpenCL to
fully control your GPU.


## A Naive Implementation of the Gaussian Blur in C++

Right, let's rewind back in time and imagine myself more than 15 years ago.
Freshly graduated with a Master's degree looking to showcase some work
portfolio. I am a bit aware of CPU registers but I can't see how it would fit in
the grand scheme of things. I've heard of vector instructions but never
understood what they were about. As an avid Internet reader, I have been sold
the big lie: don't prematurely optimize and just let the compiler do its job.
And just make things work.

I did learn in class that the Gaussian filter is separable: Don't write the 2D
convolution naively, exploit its separable property. Icing on the cake, I have
discovered multicore parallelism via OpenMP. I am proudly exhibiting a naive
implementation in C++ in my Sara repo, something along the lines below.

Below is the first step, that is the x-convolution.
```{Rcpp}
auto conv_x(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto xk = x - r + k;

        // Check the boundary conditions.
        if (xk < 0)
          xk = 0;
        else if (xk >= w)
          xk = w - 1;

        // Accumulate.
        const auto xy = y * w + xk;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

Then we perform the same dance for the y-convolution:

```{Rcpp}
auto conv_y(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto yk = y - r + k;

        // Check the boundary conditions.
        if (yk < 0)
          yk = 0;
        else if (yk >= h)
          yk = h - 1;

        // Accumulate.
        const auto xy = yk * w + x;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

I diligently write unit tests, validate on synthetic tests, check the boundary
conditions and try it on a real image with a Gaussian kernel. I am happy it
works reasonably fast when compiling in Release mode on Visual Studio. Job done!
I am proudly showcasing my code on GitHub. Never complained about it as I
bothered about real-time issues as later I learnt about in CUDA and would write
in CUDA anyways.

### Issues in the C++ code

Let's enumerate some issues and ideas that we will address later on.

1. The only parallelism we are using is the **multi-threading**.
2. It is not clear how the **CPU vector intrinsics** can be applied in the convolution.
   - For one thing, the boundary checking does not easily allow the compiler to
   vectorize the C++ code.
   - There is actually a better way to exploit the CPU vector instructions for
     the convolutional operation and the C++ code does not do this way.
3. **Data locality** is very important aspect, which we are not exploiting.

It certainly wasn't the fault of my younger self who did not know any better.
Let us now address these issues and ideas in Halide.


## A Very Fast Implementation in Halide

As we said it earlier, Halide splits an algorithm into two parts:

1. the implementation and
2. the schedule for any algorithm.

We will now detail each part. This section will divided in 3 parts.

1. First we implement the separable convolution in Halide.
2. Then we explain the different kinds of parallelisms that the CPU offers.
3. Finally we explain the schedule part of the algorithm in Halide and we will
   explore a few different schedules.

### The Algorithm

First, let us write the implementation that exploits the separability of the
Gaussian filter.

```{cpp}
#include <Halide.h>

auto x = Halide::Var{"x"};
auto y = Halide::Var{"y"};

auto input = Halide::Func{"input"};
auto kernel = Halide::Func{"kernel"};

static constexpr auto sigma = 3.f;
static constexpr auto truncation_factor = 4.f;
static constexpr auto ksz = static_cast<int>(2 * sigma * truncation_factor);
static constexpr auto kr = ksz / 2;

// N.B.: we just need to define something for the input... but just think of it
// as an image.
input(x, y) = x + y;

// The 1D Gaussian filter.
kernel(x) = Halide::exp(-x * x / (0.5f * sigma * sigma));

auto conv_x = Halide::Func{"conv_x"};
auto conv_y = Halide::Func{"conv_y"};

auto k = Halide::RDom(0, ksz, "k");

// The algorithm
conv_x(x, y) = Halide::sum(input(x + k - kr, y) * kernel(k));
conv_y(x, y) = Halide::sum(conv_x(x, y + k - kr / 2) * kernel(k));
```

### Two Types of Parallelisms on CPUs

There are two types of parallelisms on the CPU which we can exploit altogether.

1. Multicore processing:

   This is straightforward to understand. A CPU can be thought as a factory of
   workers, each one of them being called a CPU core. The multicore processing
   consists in keeping each CPU core as busy as possible with minimal data
   sharing.

   OpenMP is one implementation of multicore processing among others to
   parallelise our image filter.

2. Vector instructions:

   Until I implemented filters with Halide, I really did not understand what CPU
   vector instrutions were really about.

   Mainly, a CPU vector instruction enables a CPU core to perform arithmetic
   operations on small vectors in a **single** CPU cycle.
   That means that additions, subtractions, dot products on 4D float vectors can
   be executed in a single CPU cycle instead of 4 CPU cycles (7 CPU cycles for
   the dot product).

   That is very significant on a very large scale as we can observe additional
   4x speed up or more on very very large arrays and therefore image data.

Nowadays, an Intel CPU that supports AVX-512 vector instructions can perform
operations on 16D vectors of 32-bit floating point data in a single CPU
instruction.

So when we combine parallelism 1 and 2 on an 12-core Intel CPU with AVX-512
instructions, an optimized algorithm could theoretically be sped up by a factor
of $12 \times 16 = 192$ on 32-bit floating point array. This is huge.

Now, for more accurate and more comprehensive information, I will simply
encourage you to do your own research on ARM, RISC-V CPUs, and share what you
have learnt.

Likewise, optimized linear algebra routines like OpenBLAS makes extensive use of
the two types parallelism on CPU platforms.

We are now moving to the schedule, which is the most difficult part of the
implementation. We will explore 3 schedules with Halide.

### Schedule 1: Naive Strategy

The obvious strategy is to start from the idea of separable convolution and
vectorize the convolution wherever possible.

We parallelize the computation of image rows. We apply Halide's magic invocation
to vectorize the convolution without trying to really understand why it works.

```{cpp}
// Precompute the kernel all at once.
//
// This is irrelevant in the schedule but necessary when you want to understand
// the assembly code generated by Halide.
kernel.compute_root();

// First, compute the x-convolution in a separate memory buffer and use CPU
// vector instructions.
conv_x
    .compute_root()
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;

// Second, compute the y-convolution as a second step.
conv_y
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf) ;
```

Thanks to the CPU vectorization, this schedule is a nice improvement over the
naive C++ implementation as we exploit the CPU data vectorization. Yet OpenCV's
implementation is still better than this schedule by a factor 2.

We will worry about how the CPU vectorization is implemented later by examining
a better schedule. Let's move on to a better schedule: schedule #2.

### Schedule 2: From a Publication

In fact, the optimal schedule is really not obvious in CPU. After digging in the
publications, I find this schedule in one of Halide publications:

```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

conv_y.tile(x, y, xo, yo, xi, yi, 64, 64, Halide::TailStrategy::GuardWithIf)
    .fuse(xo, yo, tile_index)
    .parallel(tile_index);
    .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
```

This literally crushes OpenCV's implementation but I didn't understand why at
that time. However the main idea in this schedule is to better exploit data
locality but splitting the image into smaller images.

As argued in a presentation by Halide authors, without Halide, the optimal
schedule is definitely not obvious and would have necessitated at least 3 months
of work in order how to exploit CPU SIMD instructions and the data locality in
the memory cache.

Specifically this schedule the final convolved image into square tiles of 64 by
64 pixels. The set of input tiles can be seen as an input batch of many smaller
images and processed in parallel. The output image is obtained by reassembling
the batch of output image tiles.

We will detail later how the vectorization of the convolution is done.

### Schedule 3: An Improved Version by Myself

Because each output image tile is independent of each other, we can calculate
their x-convolution and y-convolution that can fit in the memory cache. For each
tile we can fit the x-convolution in the CPU cache and it improves data locality
in the memory cache. Then we exploit the CPU vector instructions to calculate
convolution in batch.


```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

auto tile = Halide::Var{"tile"};

// Precalculate the kernel.
// We want this to avoid cluttering Halide's compiled statement later on.
kernel.compute_root();

// The schedule
conv_y  //
    .tile(x, y, xo, yo, xi, yi, 64, 64)
    .fuse(xo, yo, tile)
    .parallel(tile)
    .vectorize(xi, 32, Halide::TailStrategy::GuardWithIf)  //
    ;
conv_x
    .compute_at(conv_y, xi)                               //
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)  //
    ;
```

### Second-Guessing What Halide Does

There is a lot to unpack here. Let's try to break it down bit by bit the
schedule below:

```{cpp}
  conv_y.tile(x, y, xo, yo, xi, yi, 64, 64, Halide::TailStrategy::GuardWithIf)
      .fuse(xo, yo, tile_index)
      .parallel(tile_index);
      .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
      ;
```

According to how I understand it:
```{cpp}
#pragma omp parallel for
for (auto tile_index = 0; tile_index < T; ++T)
{
  // Process the tile (xo, yo). const auto yo = tile_index / T; const auto xo =
  tile_index % T;

  for (auto yi = 0; yi < 64; ++yi)
  {
    for (auto xi = 0; xi < 64; ++yi)
    {
      const auto xi_begin = xo * 64 + xi;
      const auto xi_end = (xo + 1) * 64 + xi;

      // Recompose x and y.
      const auto y = yo * 64 + yi;
      const auto x = xo * 64 + xi;


      // compute_at(...) allocates memory on the stack as explained in:
      // https://halide-lang.org/docs/class_halide_1_1_func.html#a800cbcc3ca5e3d3fa1707f6e1990ec83.
      //
      // I understand that the storage **has** to be 2D, if we want to calculate
      // conv_y to ensure maximum data locality.
      float conv_x[ksz][ksz];

      // Trivially vectorizable.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] = 0.f;

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] += in[y][x + k] * kernel[k];

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++ksz)
        conv[y][x] = 0.f;

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++ksz)
        conv_y[y][x] += conv_x[yi+k][xi] * kernel[k];
    }
  }
}
```

It is not yet clear to me how the convolutions are vectorized in the variable
`xi`. This is the goal of the next paragraph.

### Understanding with Halide Compiled Statement

The second-guessing turns out to be not that bad but we really need to
understand what Halide does for us. How is the CPU vectorization done?

To get a more definite answer, let's actually inspect the assembly code
generated by Halide.

```{cpp}
conv_y.compile_to_stmt(
  "separable_conv_2d.stmt.html",
  {},
  Halide::HTML
);
```

Halide generates a nicely illustrated HTML document. Still The generated code is
overwhelming as there is a lot to unpack. How are we going to learn how to read
a bit of assembly code.

To digest the algorithmic flow, start by commenting out the different
parallelisms in the schedule:

1. the multicore parallelism
   ```{cpp}
   conv_y.
     ...
     // .fuse(xo, yo)
     // .parallel(tile)`
   ```
2. the vectorization parallelism
   ```{cpp}
   conv_y.
     ...
     // vectorize(xi, 32, GuardWithIf);

   conv_x.
     ...
     // vectorize(xi, 32, GuardWithIf);
   ```

This will help us to map out mentally the algorithmic roadmap and understand how
the HTML visually maps out the **three-way** correspondence between:
1. each part of the pseudo-code
2. each diagram block and
3. each of the assembly code.

After a bit of inspection, by reinstating progressively all the different
parallelisms, we start to understand how the parallel patterns are implemented
on assembly code.

This is how it looks like.
<iframe src="./random/separable_conv_2d.stmt.html" height="405" width="720"
style="border: 1px solid #464646;" allowfullscreen>
</iframe>

It is not practical to view the embedded HTML page, so go to this <a
href="./random/separable_conv_2d.stmt.html">link</a> to fully explore the
compiled statement as a full page:


### Vectorizing in the Convolution

Upon inspection of the compiled statement, it turns out that the convolution
operation is implemented by batch where we calculate 4, 8 or 16 convolved values
at the same time by repeating the vectorized fused multiply-add `fmla.4s`
operation.

For example, the vectorization the x-convolution can be translated equivalently
in NumPy code as

```{python}
import numpy as np

def convolve_vectorized(conv_x, input, kernel, tile, xi, yi):
    # Trivially vectorized initialization.
    conv_x[tile, yi, xi:xi+ksz] = 0

    # Repeat the fused multiply-add operation as follows.
    ksz = kernel.shape[0]
    for k in range(ksz):
        conv_x[tile, yi, xi: xi+ 32] = \
            conv_x[tile, yi, xi:xi + 32] + \
            input[tile, yi, xi + k:xi + 32 + k] * kernel[k]
```


References:

- https://diveintosystems.org/book/C9-ARM64/common.html
- https://developer.arm.com/documentation/
