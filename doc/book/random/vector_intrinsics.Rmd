# Super Fast Separable Convolutions

Sounds boring? I promise you this is going to be way more interesting than you
might think. While this is a common operation in low-level operation, there is
quite a few interesting things to learn how to leverage the hardware to
accelerate computing.

Story time before we dive into the implementation of the Gaussian blur.

## Story Time

Once, I applied for a C++ technical leadership role for some company. I did a
behavioral interview with their CTO. He then told me who I would be interviewing
with in the next interviews. A few days later, he called me back for a quick
chat. He ended up telling me they would not move forward after examining sara's
image processing code. Without really explaining why, the most likely reason I
could guess was that one of their senior engineers disqualified me because all
he cared about was whether I could understand and write code with CPU vector
instructions.

The CTO profusedly apologized to me. He said politely that I certainly was
gifted but my code was not up to their standard. Indeed they must have deemed that
my image processing code was implemented too naively.

During the behavioral interview, the CTO told me they were doing lots of
optimization involving CPU vector intrinsics in order to calculate data as fast
as possible. That it really was not that difficult and that I could get up to
speed fairly quickly. That was the main reason I believe they disqualified me.

Having mostly an applied math background, it did sound unfair and hypocritical
to me. Hypocritical because, if it really was easy, then why can you not learn
on the job?
So it did make me feel that you are never enough whatever you achieve in life.
Oh, so you're supposed to master every single damn thing of software engineering
from high level to low level when you start a job?
Going by the same logic, if David Lowe showcased his SIFT research code, he
would not qualify for the job either since I learnt from studying his code.

End of the rant... In hindsight I would not have been happy with the job
anyways. Back to the topic: today how can we achieve that? Nowadays, we have
some answers with Halide. And we can do it very elegantly without coding in
assembly code directly.

## What is Halide?

With Halide you can write a an image processing filter and optimize for each
specific hardware and architecture. All of this in a *very few lines of code*.
Then Halide will compile the resulting code into usable assembly code.

The main beauty with Halide is that you can decouple:

1. The algorithm: the separable convolution and,
2. the scheduling strategy: the parallelism strategy to make it as fast as the
baseline if not faster than OpenCV's Gaussian blur.

Halide can check and provide guarantees that your algorithm remains correct for
the schedule you are implementing.

With Halide, you won't write any nested `for` loops and multi-threaded
processing with bound-checking. Instead you express these ideas at a higher
level.

Halide abstracts the different kind of parallelisms for you and supports a wide
range of platforms. You don't have to be an expert in CPU vector intrinsics, but
you do need to know the schedule strategies to say optimize the speed at which
your convolutional operations run.

You still need to skim through the publications, the presentations and practise.
But in my experience, it is still difficult for the layman or the novice to
identify the schedule patterns that work and those that don't.

### Halide vs Explicit CPU intrisic code

Naysayers be like: "but Halide is too high level and does too much magic!"

Optimizing algorithms by writing explicitly CPU intrinsic instructions can
certainly be done. But you would have to pay a very costly engineering price.
You have to optimize for different CPU platforms: x86-64, ARM, RISC-V and learn
about their different intrinsic API. The resulting code is much harder to
maintain than using a unified language that allows you to write these in a very
lines of codes.

Unless this is your full-time job and something you really want to learn, that's
not something a computer vision scientist like me would like to spend time to...
Halide has done an excellent job in abstracting this at the least on the CPU
side. So know who you are and decide what you want to do.

Let's conclude this paragraph with a few words regarding the GPU acceleration
with Halide. On the GPU side, Halide is indeed not yet mature. For one thing,
the documentation is still lacking regarding the memory model at this time of
writing. You will be much better off writing code in CUDA, Vulkan, or OpenCL to
fully control your GPU.


## Naive C++ Implementation of the Gaussian Blur

Right, let's rewind back in time and imagine myself more than 15 years ago.
Freshly graduated with a Master's degree looking to showcase some work
portfolio. I am a bit aware of CPU registers but I can't see how it would fit in
the grand scheme of things. I've heard of vector instructions but never
understood what they were about. As an avid Internet reader, I have been sold
the big lie: don't prematurely optimize and just let the compiler do its job.
And just make things work.

I did learn in class that the Gaussian filter is separable: Don't write the 2D
convolution naively, exploit its separable property. Icing on the cake, I have
discovered multicore parallelism via OpenMP. I am proudly exhibiting a naive
implementation in C++ in my Sara repo, something along the lines below.

Below is the first step, that is the x-convolution.
```{Rcpp}
auto conv_x(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto xk = x - r + k;

        // Check the boundary conditions.
        if (xk < 0)
          xk = 0;
        else if (xk >= w)
          xk = w - 1;

        // Accumulate.
        const auto xy = y * w + xk;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

Then we perform the same dance for the y-convolution:

```{Rcpp}
auto conv_y(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto yk = y - r + k;

        // Check the boundary conditions.
        if (yk < 0)
          yk = 0;
        else if (yk >= h)
          yk = h - 1;

        // Accumulate.
        const auto xy = yk * w + x;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

I diligently write unit tests, validate on synthetic tests, check the boundary
conditions and try it on a real image with a Gaussian kernel. I am happy it
works reasonably fast when compiling in Release mode on Visual Studio. Job done!
I am proudly showcasing my code on GitHub. Never complained about it as I
bothered about real-time issues as later I learnt about in CUDA and would write
in CUDA anyways.


## Halide Implementation of the Algorithm

Back to the time where the CTO and his minions tell you that you are not good
enough without elaborating why and what he was expecting to see.

Fine! let's see how we could optimize the code...

I vaguely understand we have to rewrite the code explicitly with CPU intrinsics?
Should I write it in C-style or in ASM code? How to do it with limited bandwidth
after finishing your day job and wanting to learn?

Can we use Halide? Yes! It is a very elegant language. It can also compile your
code directly in OpenCL and Vulkan, Direct3D bit code. Here is how we could
rewrite the 2D separable convolution.

Let us write the implementation first.

```{cpp}
#include <Halide.h>

auto x = Halide::Var{"x"};
auto y = Halide::Var{"y"};

auto input = Halide::Func{"input"};
auto kernel = Halide::Func{"kernel"};

static constexpr auto sigma = 3.f;
static constexpr auto truncation_factor = 4.f;
static constexpr auto ksz = static_cast<int>(2 * sigma * truncation_factor);
static constexpr auto kr = ksz / 2;

input(x, y) = x + y;
kernel(x) = Halide::exp(-x * x / (0.5f * sigma * sigma));

auto conv_x = Halide::Func{"conv_x"};
auto conv_y = Halide::Func{"conv_y"};

auto k = Halide::RDom(0, ksz, "k");

// The algorithm
conv_x(x, y) = Halide::sum(input(x + k - kr, y) * kernel(k));
conv_y(x, y) = Halide::sum(conv_x(x, y + k - kr / 2) * kernel(k));
```

## Shedule Optimization

### Two Types of Parallelisms on CPUs

There are two types of parallelisms on the CPU.

1. Multicore processing:
   This is straightforward to understand and is about keep all the CPU cores as
   busy as possible with minimal data sharing. OpenMP is simple and helps to
   parallelize image filter quite easily once we identify the parts of the
   algorithm that operate independently.

2. Vector instructions:
   Until I implemented filters with Halide, I could not understand what CPU
   instrutions were really about.
   I am not going to pretend to be an expert in CPU optimization but this little
   paragraph should convince you why it is so interesting to apply vector
   instructions wherever possible.
   So, as a first approximation, a CPU vector instruction typically enables the
   programmer to perform arithmetic operations on small vectors in a single CPU
   cycle. Typically arithmetic operations such addition, multiplication and more
   can operate on 4D vectors. That is where we can observe additional 4x speed
   up or more if your CPU can process those operations on bigger vectors.

For more accurate and more comprehensive information, I will simply encourage
you to do your own research and share what you have learnt.

Like image filter, BLAS routines makes extensive use of the two types
parallelism on CPU platforms.

### Schedule 1

The obvious strategy is to start from the idea of separable convolution and
vectorize the convolution wherever possible.

We parallelize the computation of image rows. We apply Halide's magic invocation
to vectorize the convolution without really understanding why but thinks it
works.

```
kernel.compute_root();

conv_x
    .compute_root()
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;
conv_y
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;
```

Nice improvements over your naive implementation but then you decide to compete
with OpenCV just to see that it still crushes your implementation by being 2x
faster.

### Schedule 2: found in one Halide publication.

In fact, the optimal schedule is really not obvious in CPU as exposed in Halide
presentations. Until you dig into Halide publications, you start to understand
how much work and expertise it is to optimize a typical image processing filter
in Photoshop and took 3 months of hard work to correctly invoke CPU vector
instructions.

After digging in the publications, I find this schedule in one of Halide's
publications:

```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

conv_y.tile(x, y, xo, yo, xi, yi, 64, 64, Halide::TailStrategy::GuardWithIf)
    .fuse(xo, yo, tile_index)
    .parallel(tile_index);
    .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
```

This crushes OpenCV's implementation but I don't understand why.

The first step to achieve this to divide the final convolved image into tiles of
64 by 64 pixels. The set of tiles can be seen as an input batch of many smaller
images. The output is another batch of image tiles with the same image sizes
(let's just assume that the image width and height are multiple of 64.)


### Schedule 3: A better one found by myself

Because each output image tile is independent of each other, we can calculate
smaller x-convolution and y-convolution. For each tile we can fit the
x-convolution in the CPU cache and it improves data locality in the memory
cache. Then we explot the CPU vector instructions to calculate convolution in
batch.


```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

auto tile = Halide::Var{"tile"};

// Precalculate the kernel.
// We want this and this will avoid cluttering Halide's compiled statement later
// on.
kernel.compute_root();

// The schedule
conv_y  //
    .tile(x, y, xo, yo, xi, yi, 64, 64)
    .fuse(xo, yo, tile)
    .parallel(tile)
    .vectorize(xi, 32, Halide::TailStrategy::GuardWithIf)  //
    ;
conv_x
    .compute_at(conv_y, xi)                               //
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)  //
    ;
```

#### Second-Guessing what Halide does

There is a lot to unpack here. Let's try to break it down bit by bit the
schedule below:

```{cpp}
  conv_y.tile(x, y, xo, yo, xi, yi, 64, 64, Halide::TailStrategy::GuardWithIf)
      .fuse(xo, yo, tile_index)
      .parallel(tile_index);
      .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
```

According to how I understand it:
```{cpp}
#pragma omp parallel for  // .parallel(y)
for (auto tile_index = 0; tile_index < T; ++T)
{
  // Process the tile (xo, yo).
  const auto yo = tile_index / T;
  const auto xo = tile_index % T;

  for (auto yi = 0; yi < 64; ++yi)
  {
    for (auto xi = 0; xi < 64; ++yi)
    {
      const auto xi_begin = xo * 64 + xi;
      const auto xi_end = (xo + 1) * 64 + xi;

      // Recompose x and y.
      const auto y = yo * 64 + yi;
      const auto x = xo * 64 + xi;


      // Inferring from:
      // https://halide-lang.org/docs/class_halide_1_1_func.html#a800cbcc3ca5e3d3fa1707f6e1990ec83.
      //
      // Halide must allocate some storage on the stack
      //
      // I understand that the storage **has** to be 2D, if we want to calculate
      // conv_y to ensure maximum data locality.
      float conv_x[ksz][ksz];

      // TODO: verify this.
      //
      // This is trivially vectorizable.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] = 0.f;

      // Vectorizable in xi? I don't understand how.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] += in[y][x + k] * kernel[k];

      // Vectorizable in xi? I don't understand how.
      for (auto k = 0; k < ksz; ++ksz)
        conv[y][x] = 0.f;

      // Vectorizable in xi? I don't understand how.
      for (auto k = 0; k < ksz; ++ksz)
        conv_y[y][x] += conv_x[yi+k][xi] * kernel[k];
    }
  }
}
```

I still don't understand how the convolutions are vectorized in the variable
`xi`. This is the goal of the next paragraph.

### Understanding with Halide Compiled Statement

OK the second-guessing turns out to be not that bad but we really understand
what Halide does for us.

To get a more definite answer, let's pluck up the courage to actually inspect
the assembly code generated by Halide.

```{cpp}
conv_y.compile_to_stmt(
  "separable_conv_2d.stmt.html",
  {},
  Halide::HTML
);
```

Halide generates a nicely illustrated HTML document. Still The generated code is
overwhelming as there is a lot to unpack. How are we going to learn how to read
a bit of assembly code.

To digest the algorithmic flow, start by commenting out the different
parallelisms in the schedule:

1. the multicore parallelism
   ```{cpp}
   conv_y.
     ...
     // .fuse(xo, yo)
     // .parallel(tile)`
   ```
2. the vectorization parallelism
   ```{cpp}
   conv_y.
     ...
     // vectorize(xi, 32, GuardWithIf);

   conv_x.
     ...
     // vectorize(xi, 32, GuardWithIf);
   ```

This will help us to map out mentally the algorithmic roadmap and understand how
the HTML visually maps out the **three-way** correspondence between:
1. each part of the pseudo-code
2. each diagram block and
3. each of the assembly code.

After a bit of inspection, by reinstating progressively all the different
parallelisms, we start to understand how the parallel patterns are implemented
on assembly code.

This is how it looks like.
<iframe src="./random/separable_conv_2d.stmt.html" height="405" width="720"
style="border: 1px solid #464646;" allowfullscreen>
</iframe>

It is not practical to view the embedded HTML page, so go to this <a
href="./random/separable_conv_2d.stmt.html">link</a> below to fully explore the
compiled statement as a full page:


### Vectorizing in the Convolution

Eventually, this was not so hard and this gives a very tangible hands-on
introduction to assembly code.

Then it becomes clear that the convolution operation is implemented by batch
where we calculate 4, 8 or 16 convolved values at the same time by repeating
the vectorized fused multiply-add `fmla.4s` operation.

The vectorization the x-convolution can be translated in terms of NumPy equivalent code

```
import numpy as np

def convolve_vectorized(conv_x, in, kernel, tile, xi, yi):
    # Trivially vectorizable initialization.
    conv_x[tile, yi, xi:xi+ksz] = 0

    # Repeat the fused multiply-add operation as follows.
    ksz = kernel.shape[0]
    for k in range(ksz):
        conv_x[tile, yi, xi:xi+32] = conv_x[tile, yi, xi:xi+32] \
                                   + in[tile, yi, xi:xi+32] * kernel[k]
```


References:

- https://diveintosystems.org/book/C9-ARM64/common.html
- https://developer.arm.com/documentation/
