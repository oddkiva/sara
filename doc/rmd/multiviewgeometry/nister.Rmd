# Relative Pose Estimation

In this chapter, I will decribe Nister's method which was originally published
in CVPR [@nister:2003:cvpr] and republished at length in PAMI 
[@nister:2004:pami]. It solves the essential matrix from $5$ point 
correspondences. Here I revisit the PAMI article [@nister:2004:pami] which
I found clearer.

The method is very technical as it is purely based on simple algebraic
manipulations. As such, the paper may seem boring but I believe understanding
these such details are useful.

## Underdetermined System of Linear Equations

Let $\left\{\mathbf{p}_i, \mathbf{q}_i \right\}_{0 \leq i < 5}$ denote a set of
$5$ point correspondences, with $\mathbf{p}_i$ and $\mathbf{q}_i$ being points
on the left image and the right image respectively (in homogeneous coordinates).

They satisfy the epipolar constraints:

\begin{equation}
  \bq_i^T \bE \bp_i = 0
\end{equation}

Let $\be$ denote the essential matrix in its row-major vectorized
form, *i.e.*,

\begin{equation}
  \be = \mathrm{vec}(\bE)
\end{equation}

The $\mathrm{vec}(.)$ operator corresponds to the operation `E.flatten()` in
NumPy.

Expanding the system of five equations, we have equivalently:

\begin{equation}
  \bQ \be = 0
\end{equation}

with each row of $\bQ$ defined as

\begin{equation}
  \mathbf{Q}_i =  \mathrm{vec}(\mathbf{p}_{i} \mathbf{q}_i^T)
\end{equation}

$\be$ lives in the nullspace of $\mathbf{Q}$ which is of
dimension $4$. And the nullspace depends on the $5$ point
correspondences.

## Nullspace Decomposition

Using a singular value decomposition allows us to express the nullspace
$\mathrm{Null}(\mathbf{Q})$ as a span of 4 basis vectors $(\mathbf{x},
\mathbf{y}, \mathbf{z}, \mathbf{w})$

\begin{equation}
  \mathrm{Null}(\bQ) = \mathrm{span}(\x, \y, \z, \w),
\end{equation}

which allows us to express the vectorized essential matrix $\be$ as a linear
combination of these $4$ basis vectors

\begin{equation}
  \be = x \x + y \y + z \z + \w.
\end{equation}

The goal is to determine the value of the real scalars $x, y, z$.

## Essential Matrix Constraints

:::fyi
Before proceeding onwards, let us mention a notational peculiarity used to
denote polynomials in [@nister:2004:pami]. Perhaps like me, you may find it very
confusing that angular brackets $\langle p \rangle$ are used instead of the
usual notation like $p(x, y, z)$ to denote a polynomial $p$. Understandably, the
latter is a lot heavier than the former and this may distract the flow in the
mathematical discourse.

Here we reuse the same notation so that you can go back to the original paper
and check again without additional mental overhead.
:::

As mentioned in [@nister:2004:pami]:

1. the essential matrix is also characterized equivalently by the matrix
   equation:

   \begin{equation}
     \begin{bmatrix}
     \langle e_{00} \rangle & \langle e_{01} \rangle & \langle e_{02} \rangle \\
     \langle e_{10} \rangle & \langle e_{11} \rangle & \langle e_{12} \rangle \\
     \langle e_{20} \rangle & \langle e_{21} \rangle & \langle e_{22} \rangle \\
     \end{bmatrix}
     \iff
     \bE \bE^T \bE - \frac{1}{2} \mathrm{trace}(\bE\bE^T) \bE = 0
   \end{equation}

   This gives $3 \times 3 = 9$ polynomial equations $\langle e_{ij}
   \rangle$ in the monomials $x^\alpha y^\beta z^\gamma$ where
   $\alpha + \beta + \gamma \leq 3$.

2. the essential matrix $\bE$ has rank 2, so its determinant must be
   zero:

   \begin{equation}
     \langle d \rangle \iff \mathrm{det}(\bE) = 0
   \end{equation}

   This gives another polynomial equation $\langle d \rangle$ in the
   monomials $x^\alpha y^\beta z^\gamma$.

So we have in total $10$ polynomial equations in the monomials
$x^\alpha y^\beta z^\gamma$. There are 20 monomials in total.

[@nister:2004:pami] enumerates these monomials in the following order:

\begin{equation}
  \scriptsize
  %
  \begin{array}{|c|cccccccccc|ccc|ccc|cccc|}
  \hline
    &
  x^3 & y^3 & x^2 y & x y^2 & x^2 z & x^2 & y^2 z & y^2 & xyz & xy &
  x & x z & x z^2 & y & y z & y z^2 & 1 & z & z^2 & z^3 \\
  %
  \hline
  %
  \langle e_{00} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{01} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{02} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{10} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{11} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{12} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{20} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{21} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \langle e_{22} \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \hline
  %
  \langle d \rangle &
  . & . & . & . & . & . & . & . & . & . &
  . & . & . & . & . & . & . & . & . & . \\
  %
  \hline
  \end{array}
\end{equation}

Contrary to later papers, [@nister:2004:pami] does not use Groebner basis to
solve the system of $10$ polynomial equations. Rather he uses only elementary
algebraic operations. This is very neat as we do not need to learn a new
mathematical theory to solve this specific geometric problem. We detail how he
does it.

## Gauss-Jordan Elimination

:::note
This subsection is the trickiest part to understand in the method. Let us detail
a bit more the technical details provided by the paper.

In a nutshell, we explain how:

- from this set of $10$ polynomial equations that are simultaneously satisfied
  implies to solving only a set of $3$ easier polynomial equations;
- this set of $3$ equations is equivalently reduced to a polynomial that depends
  of only one variable and of degree $10$.

Now, from the point of view of mathematical rigor, it is entirely not clear if
the initial system of $10$ polynomial equations has at most only 10 possible
solutions but in practice this method gives good results.
:::

Using the Gauss-Jordan elimination we can reduce the system of polynomial
equation so that the lower-triangular part of the left matrix block above is
zero everywhere except one on the diagonal.

We will realize that it is actually sufficient to apply the Gauss-Jordan
elimination. Specifically,

1. perform the full sweep downward so that lower diagonal part
   is fully zero

   \begin{equation}
     \scriptsize
     \begin{array}{|c|cccccccccc|ccc|ccc|cccc|}
     \hline
     %
     &
     x^3 & y^3 & x^2 y & x y^2 & x^2 z & x^2 & y^2 z & y^2 & xyz & xy &
     x & x z & x z^2 & y & y z & y z^2 & 1 & z & z^2 & z^3 \\
     %
     \hline
     %
     \langle e_{00} \rangle &
     1 & . & . & . & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{01} \rangle &
     0 & 1 & . & . & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{02} \rangle &
     0 & 0 & 1 & . & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{10} \rangle &
     0 & 0 & 0 & 1 & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{11} \rangle &
     0 & 0 & 0 & 0 & 1 & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{12} \rangle &
     0 & 0 & 0 & 0 & 0 & 1 & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{20} \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 1 & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{21} \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{22} \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \hline
     %
     \langle d \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \hline
     \end{array}
     %
     (\#eq:sweep-downward)
   \end{equation}

2. then in the sweep upward, stop halfway until the system of polynomial
   equations looks like the system of equations \@ref(eq:gaussjordan) below:

   \begin{equation}
     \scriptsize
     \begin{array}{|c|cccccccccc|ccc|ccc|cccc|}
     \hline
     %
     &
     x^3 & y^3 & x^2 y & x y^2 & x^2 z & x^2 & y^2 z & y^2 & xyz & xy &
     x & x z & x z^2 & y & y z & y z^2 & 1 & z & z^2 & z^3 \\
     %
     \hline
     %
     \langle e_{00} \rangle &
     1 & . & . & . & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{01} \rangle &
     0 & 1 & . & . & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{02} \rangle &
     0 & 0 & 1 & . & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{10} \rangle &
     0 & 0 & 0 & 1 & . & . & . & . & . & . &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{11} \rangle &
     0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{12} \rangle &
     0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{20} \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{21} \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \langle e_{22} \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \hline
     %
     \langle d \rangle &
     0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 &
     . & . & . & . & . & . & . & . & . & . \\
     %
     \hline
     \end{array}
     %
     (\#eq:gaussjordan)
   \end{equation}

Let us look again at the last $6$ equations \@ref(eq:gaussjordan). *The final
key set of algebraic operations is to reduce those $6$ polynomials by
multiplying by $z$ and subtracting as follows:*

\begin{equation}
  \begin{aligned}
  \langle k \rangle &= \langle e_{11} \rangle - z \langle e_{12} \rangle \\
  \langle l \rangle &= \langle e_{20} \rangle - z \langle e_{21} \rangle \\
  \langle m \rangle &= \langle e_{22} \rangle - z \langle d \rangle
  \end{aligned}
  %
  (\#eq:klm)
\end{equation}

So that in the end the system of $3$ equations \@ref(eq:klm) depends only the
following on the last $10$ monomials on the right. We can group them as follows:

- $\left\{ x, x z, x z^2, x z^3 \right\}$
- $\left\{ y, y z, y z^2, x z^3 \right\}$
- $\left\{ 1, z, z^2, z^3, z^4 \right\}$

Let's look again at these 3 groups of monomials: the very nice thing about these
is that when we group coefficients using these three subgroups, we see
respectively that:

- coefficients in the variable $x$ form a polynomial in $z$ of degree $3$,
- coefficients in the variable $y$ form a polynomial in $z$ of degree $3$,
- coefficients in the variable $z$ form a polynomial in $z$ of degree $4$.

This is very interesting because the system of polynomial equations
\@ref(eq:klm) is rewritten in matrix form as

\begin{equation}
  \begin{array}{|c|ccc|}
  \hline
  \mathbf{B} & x & y & 1 \\
  \hline
  \langle k \rangle & [3] & [3] & [4] \\
  \langle l \rangle & [3] & [3] & [4] \\
  \langle m \rangle & [3] & [3] & [4] \\
  \hline
  \end{array}
  %
  \begin{bmatrix} x \\ y \\ 1 \end{bmatrix} = \mathbf{0}_3
  (\#eq:B)
\end{equation}

:::fyi
Once again, the notations that are used above to characterize coefficients of
matrix $\mathbf{B}$ confused me a lot. For the sake of brevity,
[@nister:2004:pami] only wants to emphasize the polynomial degree of each
coefficient.
:::

From this matrix notation, it appears that $[x, y, 1]^T$ is a vector in the
nullspace of $\mathbf{B}$ where the coefficients are polynomials in the variable
$z$ of degree $3$ or $4$.

:::puzzle
Equivalently, that means that the determinant of $\mathbf{B}$ must be zero. The
determinant is a polynomial only in the variable $z$ and of degree $10$. Hence
we can extract the roots of this polynomial with any polynomial solvers.

\begin{equation}
  \langle n \rangle \iff \mathrm{det}(\mathbf{B}) = 0
  (\#eq:n)
\end{equation}
:::

Just like in the section about ellipse intersection \@ref(ellipse-intersection),
the discovery of such clever set of algebraic operations leads to extracting the
roots of a determinant.

### Real Root Extraction

Instead of using Sturm sequences, we extract the roots of the polynomials using
Jenkins-Traub algorithm.

### Recovering the Essential Matrices

We get at most 10 possible roots $z_i$ from the polynomial root solver.
Keeping only the real roots, the cofactors of the matrix $\mathbf{B}$
allow us to retrieve the corresponding scalars $x_i$ and $y_i$. From
there we recover all the candidate essential matrices by linear combination as
exposed above.

The essential matrices that are physically valid are those who are cheiral.
