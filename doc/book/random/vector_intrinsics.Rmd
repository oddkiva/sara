# Super Fast Separable Convolutions

Sounds boring? I promise you this is going to be way more interesting than you
might think. While this is a common operation in low-level operation, there is
quite a few interesting things to learn how to leverage the hardware to
accelerate computing.

Story time before we dive into the implementation of the Gaussian blur.

## Story Time

Once, I applied for a C++ technical leadership role for some company. I did a
behavioral interview with their CTO. He then told me who I would be interviewing
with in the next interviews. A few days later, he called me back for a quick
chat. He ended up telling me they would not move forward after examining
*Sara*'s image processing code. Without really explaining why, the most likely
reason I could come up with was that one of their senior engineers disqualified
me because all he cared about was whether I could understand and write code with
CPU vector instructions.

The CTO profusedly apologized to me. He said politely that I certainly was
gifted but my code was not up to their standard. They must have deemed that my
image processing code was implemented too naively.

Indeed I remembered that during the behavioral interview, the CTO told me they
were doing lots of optimization involving CPU vector instructions in order to
process data as fast as possible. That it really was not that difficult and that
I could get up to speed fairly quickly, and blah blah blah... That was the only
reason for which, I believe, they disqualified me.

Having mostly an applied math background, it did sound unfair and hypocritical
to me. Hypocritical because, if it really was easy, then why can you not learn
it on the job?
So, yes, it did make me feel that you are never enough whatever you achieve in
life.
Oh, so you're supposed to master every single damn thing of software engineering
when you start a job?
Going by the same logic, if David Lowe showcased his SIFT research code, he
would not qualify for the job either since I learnt from studying his code.

In hindsight I would not have been happy with the job anyways. End of the rant
and back to the topic: today how can we achieve that? Nowadays, we have some
answers with Halide, which allows us to do it very elegantly.

## What is Halide?

In a technical jargon, Halide is an domain specific language embedded in C++.
The specific domain we are dealing is image processing. It is a language because
it compiles your optimized algorithm into assembly code.

With Halide, you can write an image processing filter and optimize for each
specific hardware and architecture. All of this elegangtly in a *very few lines
of code*. Then Halide will compile the C++ code into usable assembly code.

The main beauty with Halide is that you decouple:

1. the **algorithm**: in our case, the **separable convolution** and,
2. the **schedule**: which exploits the different **kinds of parallelism** that
   the hardware offers,

so that our implementation is as fast as the baseline implementation. In our
case, the baseline implementation is OpenCV's Gaussian blur.

Halide abstracts and unifies the **common programming patterns** that arise from
CPU to GPU programming in CUDA, OpenCL, Metal, Vulkan. Besides, an image
processing algorithm can often expressed as a sequence of CUDA kernels, you
don't need to write any nested loops.

Halide abstracts different kinds of parallelism and supports a wide range of
platforms. While we don't have to know the C API to invoke vector instructions,
we still need to know those common programming patterns that involves CPU vector
instructions.

When I began learning Halide, I knew very little about optimal schedules.
Skimming through publications and the presentations by Halide authors was the
only way for me to really learn. And of course practise, practise, practise. So
all in all, in my experience, the entry barrier is still very high for the
average programmer to identify which schedules work best and those that don't.

### Halide vs Explicit CPU intrisic code

Naysayers will argue: "But Halide is too high level and does too much magic!"

Certainly, optimizing algorithms by writing explicitly CPU vector instructions
can be done. But we would have to pay a very costly engineering price.

You would have to optimize for different CPU platforms: *x86-64*, *ARM*,
*RISC-V* and learn their respective C API to utilize hardware SIMD instructions.
The resulting code would be much lengthier and harder to maintain than using a
unified language that allows you to write these in a very few lines of codes.

Unless this is your full-time job or it's something you really want to learn,
personally I don't want to spend time into this. Halide has done an excellent
job in abstracting this at the least on the CPU side. So know who you are and
decide what you want to do.

Let's conclude this paragraph with a few words regarding the GPU acceleration
with Halide. On the GPU side, Halide is indeed not yet mature. For one thing,
the documentation is still lacking regarding the memory model at this time of
writing. You will be much better off writing code in CUDA, Vulkan, or OpenCL to
fully control your GPU.


## A Naive Implementation of the Gaussian Blur in C++

Right, let's rewind back in time and imagine myself more than 15 years ago.
Freshly graduated with a Master's degree looking to showcase some work
portfolio. I am a bit aware of CPU registers but I can't see how it would fit in
the grand scheme of things. I've heard of vector instructions but never
understood what they were about. As an avid Internet reader, I have been sold
the big lie: don't prematurely optimize and just let the compiler do its job.
And just make things work.

I did learn in class that the Gaussian filter is separable: Don't write the 2D
convolution naively, exploit its separable property. Icing on the cake, I have
discovered multicore parallelism via OpenMP. I am proudly exhibiting a naive
implementation in C++ in my *Sara* repo, something along the lines below.

### The x-convolution

Below is the first step, that is the x-convolution.
```{Rcpp}
#include <algorithm>

auto conv_x(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;

      // Calculate the convolved value.
      for (auto k = 0; k < ksz; ++k)
      {
        // Check the boundary conditions.
        const auto xk = std::clamp(x - r + k, 0, w - 1);

        // Accumulate.
        const auto xy = y * w + xk;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

### The y-convolution

Then we perform the same dance for the y-convolution in a similar fashion.

```{Rcpp}
#include <algorithm>

auto conv_y(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;

      // Calculate the convolved value.
      for (auto k = 0; k < ksz; ++k)
      {
        // Check the boundary conditions.
        const auto yk = std::clamp(y - r + k, 0, w - 1);

        // Accumulate.
        const auto xy = yk * w + x;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

I diligently write unit tests, validate on synthetic tests, check the boundary
conditions and try it on a real image with a Gaussian kernel. I am happy it
works reasonably fast when compiling in Release mode on Visual Studio. Job done!
I am proudly showcasing my code on GitHub. Never complained about it as I never
bothered about real-time considerations. Instead I would port the code to make
the algorithm run faster.

### Issues in the C++ code

Let's enumerate some issues and ideas that we will address later on.

1. The only parallelism we are using is the **multi-threading**.

   Each row is processed in parallel but can we do better than parallel row
   processing?

2. It is not clear how the **CPU vector instructions** can be applied in the
   convolution.
   - For one thing, the boundary checking does not easily allow the compiler to
   vectorize the C++ code.
   - There is actually a better way to exploit the CPU vector instructions for
     the convolutional operation and the C++ code does not do this way.
3. **Data locality** is a very important aspect, which the parallel row
   processing does not easily allow to leverage.

It certainly wasn't the fault of my younger self who did not know any better.
Let us now address these issues and ideas in Halide.


## A Very Fast Implementation in Halide

As we said it earlier, Halide splits an algorithm into two parts:

1. the implementation and
2. the schedule for any algorithm.

We will now detail each part. This section will divided in 3 parts.

1. First we implement the separable convolution in Halide.
2. Then we explain the different kinds of parallelisms that the CPU offers.
3. Finally we explain the schedule part of the algorithm in Halide and we will
   explore a few different schedules.

### The Algorithm

First, let us write the implementation that exploits the separability of the
Gaussian filter.

```{cpp}
#include <Halide.h>

auto x = Halide::Var{"x"};
auto y = Halide::Var{"y"};

auto input = Halide::Func{"input"};
auto kernel = Halide::Func{"kernel"};

static constexpr auto sigma = 3.f;
static constexpr auto truncation_factor = 4.f;
static constexpr auto ksz = static_cast<int>(2 * sigma * truncation_factor);
static constexpr auto kr = ksz / 2;

// N.B.: we just need to define something for the input... but just think of it
// as an image.
input(x, y) = x + y;

// The 1D Gaussian filter.
kernel(x) = Halide::exp(-x * x / (0.5f * sigma * sigma));

auto conv_x = Halide::Func{"conv_x"};
auto conv_y = Halide::Func{"conv_y"};

auto k = Halide::RDom(0, ksz, "k");

// The algorithm
conv_x(x, y) = Halide::sum(input(x + k - kr, y) * kernel(k));
conv_y(x, y) = Halide::sum(conv_x(x, y + k - kr / 2) * kernel(k));
```

### Two Types of Parallelisms on CPUs

There are two types of parallelisms on the CPU which we can exploit altogether.

1. Multicore processing:

   This is straightforward to understand. A CPU can be thought as a factory of
   workers, each one of them being called a CPU core.

   The multicore processing becomes most effective when each CPU core is being
   kept as busy as possible and the CPU cores don't need to communicate with
   each other via data synchronization.

   OpenMP is one implementation of multicore processing among others to
   parallelise our image filter.

2. Vector instructions:

   Until I implemented filters with Halide, I really did not understand what CPU
   vector instrutions were really about.

   Mainly, a CPU vector instruction enables a CPU core to perform arithmetic
   operations on small vectors in a **single** CPU cycle.
   That means that additions, subtractions, dot products on 4D float vectors can
   be executed in a single CPU cycle instead of 4 CPU cycles (7 CPU cycles for
   the dot product).

   That is very significant on a very large scale as we can observe additional
   4x speed up or more on very very large arrays and therefore image data.

Nowadays, an Intel CPU that supports AVX-512 vector instructions can perform
operations on 16D vectors of 32-bit floating point data in a single CPU
instruction.

So when we combine parallelism 1 and 2 on an 12-core Intel CPU with AVX-512
instructions, an optimized algorithm could in principle be sped up by a factor
of

\begin{equation}
  \#\{\text{CPUs}\} \times \text{SIMD max dim} = 12 \times 16 = 192
\end{equation}

on single-precision floating point data.

While we should instead use Amdahl's law instead of this naive formula, applying
Amdahl's law is not so simple and we will be content to just benchmark and
measure the performance gain.

Likewise, optimized linear algebra routines like OpenBLAS makes extensive use of
the two types parallelism on CPU platforms.

For more accurate and more comprehensive information, I will simply encourage
you to do your own research on ARM, RISC-V CPUs, and share what you have learnt.

We are now moving to the schedule, which is the most difficult part of the
implementation. We will explore 3 schedules with Halide.

### Schedule 1: Naive Strategy

The obvious strategy is to start from the idea of separable convolution and
vectorize the convolution wherever possible.

We parallelize the computation of image rows. We apply Halide's magic invocation
to vectorize the convolution without trying to really understand why it works.

```{cpp}
// Precompute the kernel all at once.
//
// This is irrelevant in the schedule but necessary when you want to understand
// the assembly code generated by Halide.
kernel.compute_root();

// First, compute the x-convolution in a separate memory buffer and use CPU
// vector instructions.
conv_x
    .compute_root()
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;

// Second, compute the y-convolution as a second step.
conv_y
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf) ;
```

This schedule is a nice improvement over the naive C++ implementation as we
exploit the CPU data vectorization. Yet OpenCV's implementation is still better
than this schedule by a factor 2.

Later we will examine how the CPU vectorization is implemented by inspecting a
better schedule. So let's move on to a better schedule: schedule #2.

### Schedule 2: From a Publication

The optimal schedule is really not obvious in CPU, at least for me. After
digging in the publications, I found this schedule in one of Halide
publications:

```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

auto tile_index = Halide::Var{"t"};

conv_y.tile(x, y, xo, yo, xi, yi, 64, 64,
            Halide::TailStrategy::GuardWithIf)
    .fuse(xo, yo, tile_index)
    .parallel(tile_index);
    .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
```

This literally crushes OpenCV's implementation but I didn't understand why at
that time. However the main idea in this schedule is to better exploit data
locality but splitting the image into smaller images.

As argued in a presentation by Halide authors, without Halide, the optimal
schedule is definitely not obvious and would have necessitated at least 3 months
of work in order how to exploit CPU SIMD instructions and the data locality in
the memory cache.

Specifically this schedule the final convolved image into square tiles of 64 by
64 pixels. The set of input tiles can be seen as an input batch of many smaller
images and processed in parallel. The output image is obtained by reassembling
the batch of output image tiles.

We will detail later how the vectorization of the convolution is done.

### Schedule 3: An Improved Version by Myself

Because each output image tile is independent of each other, we can calculate
their x-convolution and y-convolution that can fit in the memory cache. For each
tile we can fit the x-convolution in the CPU cache and it improves data locality
in the memory cache. Then we exploit the CPU vector instructions to calculate
convolution in batch.


```{cpp}
auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};

auto tile = Halide::Var{"tile"};

// Precalculate the kernel.
// We want this to avoid cluttering Halide's compiled statement later on.
kernel.compute_root();

// The schedule
conv_y  //
    .tile(x, y, xo, yo, xi, yi, 64, 64)
    .fuse(xo, yo, tile)
    .parallel(tile)
    .vectorize(xi, 32, Halide::TailStrategy::GuardWithIf)  //
    ;
conv_x
    .compute_at(conv_y, xi)                               //
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)  //
    ;
```

### Second-Guessing What Halide Does

There is a lot to unpack here. Let's try to break it down bit by bit the
schedule below:

```{cpp}
  conv_y.tile(x, y, xo, yo, xi, yi, 64, 64,
              Halide::TailStrategy::GuardWithIf)
      .fuse(xo, yo, tile_index)
      .parallel(tile_index);
      .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
      ;
```

According to how I understand it:
```{cpp}
#pragma omp parallel for
for (auto tile_index = 0; tile_index < T; ++T)
{
  // Process the tile (xo, yo).
  const auto yo = tile_index / T;
  const auto xo = tile_index % T;

  for (auto yi = 0; yi < 64; ++yi)
  {
    for (auto xi = 0; xi < 64; ++yi)
    {
      const auto xi_begin = xo * 64 + xi;
      const auto xi_end = (xo + 1) * 64 + xi;

      // Recompose x and y.
      const auto y = yo * 64 + yi;
      const auto x = xo * 64 + xi;


      // compute_at(...) allocates memory on the stack as explained in:
      // https://halide-lang.org/docs/class_halide_1_1_func.html#a800cbcc3ca5e3d3fa1707f6e1990ec83.
      //
      // I understand that the storage **has** to be 2D, if we want to calculate
      // conv_y to ensure maximum data locality.
      float conv_x[ksz][ksz];

      // Trivially vectorizable.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] = 0.f;

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] += in[y][x + k] * kernel[k];

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++ksz)
        conv[y][x] = 0.f;

      // Vectorizable in xi? How?
      for (auto k = 0; k < ksz; ++ksz)
        conv_y[y][x] += conv_x[yi+k][xi] * kernel[k];
    }
  }
}
```

It is not yet clear to me how the convolutions are vectorized in the variable
`xi`. This is the goal of the next paragraph.

### Understanding with Halide Compiled Statement

The second-guessing turns out to be not that bad but we really need to
understand what Halide does for us. How is the CPU vectorization done?

To get a more definite answer, let's actually inspect the assembly code
generated by Halide.

```{cpp}
conv_y.compile_to_stmt(
  "separable_conv_2d.stmt.html",
  {},
  Halide::HTML
);
```

Halide generates a nicely illustrated HTML document. Still The generated code is
overwhelming as there is a lot to unpack. How are we going to learn how to read
a bit of assembly code.

To digest the algorithmic flow, start by commenting out the different
parallelisms in the schedule:

1. the multicore parallelism
   ```{cpp}
   conv_y.
     ...
     // .fuse(xo, yo)
     // .parallel(tile)`
   ```
2. the vectorization parallelism
   ```{cpp}
   conv_y.
     ...
     // vectorize(xi, 32, GuardWithIf);

   conv_x.
     ...
     // vectorize(xi, 32, GuardWithIf);
   ```

This will help us to break down the algorithmic roadmap and understand how the
HTML visually maps out the **three-way** correspondence between:

1. each part of the pseudo-code,
2. each diagram block and
3. each part of the assembly code.

After a bit of inspection, by reinstating progressively all the different
parallelisms, we start to understand how the parallel patterns are implemented
on assembly code.

This is how it looks like.
<iframe src="./random/separable_conv_2d.stmt.html" height="405" width="720"
style="border: 1px solid #464646;" allowfullscreen>
</iframe>

It is not practical to view the embedded HTML page, so go to this <a
href="./random/separable_conv_2d.stmt.html">link</a> to fully explore the
compiled statement as a full page:


### Vectorizing in the Convolution

Upon inspection of the compiled statement, it turns out that the convolution
operation is implemented by batch where we calculate 4, 8 or 16 convolved values
at the same time by repeating the vectorized fused multiply-add `fmla.4s`
operation.

For example, the vectorization the x-convolution can be translated equivalently
in NumPy code as

```{python}
import numpy as np

def convolve_vectorized(conv_x, input, kernel, tile, xi, yi):
    # Trivially vectorized.
    conv_x[tile, yi, xi:xi+ksz] = 0

    # Repeat the fused multiply-add operation as follows.
    ksz = kernel.shape[0]
    for k in range(ksz):
        conv_x[tile, yi, xi: xi+ 32] = \
            conv_x[tile, yi, xi:xi + 32] + \
            input[tile, yi, xi + k:xi + 32 + k] * kernel[k]
```


References:

- https://diveintosystems.org/book/C9-ARM64/common.html
- https://developer.arm.com/documentation/
