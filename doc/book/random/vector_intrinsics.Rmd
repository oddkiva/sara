# Super Fast Separable Convolutions

Sounds boring? I promise you this is going to be way more interesting than you
might think. There's quite a bit interesting things to learn.

Story time before we dive into the implementation of the Gaussian blur.

## Story Time

Once I applied for a C++ technical leadership role for some company. I was
considered for the role after a preliminary behavioral screening by the CTO. He
then told me who I would be interviewing with in the next interviews. A few days
later, he called me back for a quick chat. He ended up telling me they would not
move forward after examining sara's image processing code. Without really
explaining why, I guessed that probably one of their senior engineers
disqualified me as he did care more about my ability in understanding and
manipulating CPU vector instructions.

The CTO profusedly apologized to me. He said politely that I certainly was
gifted but my C++ code was not to their standard. From what I guessed, they
probably found that my image processing code was implemented too naively.

This was based on the fact that the CTO told me they were doing lots of
optimization involving CPU vector intrinsics in order to calculate data as fast
as possible. That it really was not that difficult and that I could get up to
speed fairly quickly.

I have mostly an applied math background, so it did sound unfair to me. It did
made me feel that you are never enough whatever you achieve. Like you do need to
know every single aspect of engineering from high level to low level. In
hindsight I would not have been happy with the job anyways. Still frustrating...
In that moment, I was telling myself: what can you do when you are already being
shut the door? Going by the same logic, if David Lowe showcased his SIFT
research code, he would not qualify for the job either: I learnt from studying
his code.

Right, back to the topic: today how can we achieve that? Nowadays, we have some
answers with Halide. And we can do it very elegantly without coding in assembly
directly.

## What is Halide?

With Halide you can write a an image processing filter and optimize the way it is run.

You write sets of arithmetic instructions that operates on image buffers with
specific parallelism patterns (multicore and vectorization). Then you can tell
to compile with a C++ method to generate the optimize the image filter as a C++
static library.

The main beauty with Halide is that you can decouple:

1. The algorithm: the separable convolution and,
2. the scheduling strategy: the parallelism strategy to make it as fast as the
baseline if not faster than OpenCV's Gaussian blur.

Halide can check and provide guarantees that your algorithm remains correct for
the schedule you are implementing.

With Halide, you won't write any nested `for` loops and multi-threaded
processing with bound-checking. So you can express ideas at a higher level.

Halide abstracts these parallelisms for you and supports a wide range of
platforms. You don't have to be an expert in CPU vector intrinsics, but you do
need to know the schedule strategies to say optimize the speed at which your
convolutional operations run. Halide has identified the most common schedule
patterns that are used to optimize image processing code.

You still need to skim the publications and the presentations, practise. But in
my experience, it is still difficult for the layman or the novice to identify
the schedule patterns that work and those that don't.


## Naive C++ Implementation of the Gaussian Blur

Right, let's rewind back in time and imagine myself more than 15 years ago.
Freshly graduated with a Master's degree looking for an internship. I am a bit
aware of CPU registers but I can't see how it would fit in the grand scheme of
things. I've heard of vector instructions but never understood what they were
about. As an avid Internet reader, I have been sold the big lie: don't
prematurely optimize and just let the compiler do its job. And just make things
work.

I did learn in class that the Gaussian filter is separable: Don't write the 2D
convolution naively, exploit its separable property. I am proudly exhibiting a
naive implementation in C++ in my Sara repo, something along the lines:

Icing on the cake, I have discovered multicore parallelism via OpenMP.

```{Rcpp}
auto conv_x(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto xk = x - r + k;

        // Check the boundary conditions.
        if (xk < 0)
          xk = 0;
        else if (xk >= w)
          xk = w - 1;

        // Accumulate.
        const auto xy = y * w + xk;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

We execute the same dance for the y-convolution:

```{Rcpp}
auto conv_y(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;

#ifdef _OPENMP
#pragma omp parallel for
#endif
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto yk = y - r + k;

        // Check the boundary conditions.
        if (yk < 0)
          yk = 0;
        else if (yk >= h)
          yk = h - 1;

        // Accumulate.
        const auto xy = yk * w + x;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

I diligently write unit tests, validate on synthetic tests, check the boundary
conditions and try it on a real image with a Gaussian kernel. I am happy it
works reasonably fast when compiling in Release mode on Visual Studio. Job done!
I am proudly showcasing my code on GitHub. Never complained about it as I
bothered about real-time issues as later I learnt about in CUDA and would write
in CUDA anyways.


## Halide Implementation of the Algorithm

Back to the time where the CTO and his minions tell you that you are not good
enough without elaborating why and what he was expecting to see.

Fine! let's see how we could optimize the code...

OK How? I vaguely understand you have to write the code with CPU intrinsics?
Should I write in C-style or in ASM. How to do it with limited bandwidth after
finishing your day job and wanting to learn?

Use Halide? Yes! It is a very elegant language. It can also compile your code
directly in OpenCL and Vulkan, Direct3D bit code. Here is how we could rewrite
the 2D separable convolution.

Let us write the implementation first.

```{cpp}
#include <Halide.h>

auto x = Halide::Var{"x"};
auto y = Halide::Var{"y"};

auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};
auto tile = Halide::Var{"tile"};

auto input = Halide::Func{"input"};
auto kernel = Halide::Func{"kernel"};

static constexpr auto sigma = 3.f;
static constexpr auto truncation_factor = 4.f;
static constexpr auto ksz = static_cast<int>(2 * sigma * truncation_factor);
static constexpr auto kr = ksz / 2;

input(x, y) = x + y;
kernel(x) = Halide::exp(-x * x / (0.5f * sigma * sigma));

auto conv_x = Halide::Func{"conv_x"};
auto conv_y = Halide::Func{"conv_y"};

auto k = Halide::RDom(0, ksz, "k");

// The algorithm
conv_x(x, y) = Halide::sum(input(x + k - kr, y) * kernel(k));
conv_y(x, y) = Halide::sum(conv_x(x, y + k - kr / 2) * kernel(k));
```

## Shedule Optimization

### Two Types of Parallelisms on CPUs

There are two types of parallelisms on the CPU.

1. Multicore processing:
   This is straightforward to understand and is about keep all the CPU cores as
   busy as possible with minimal data sharing. OpenMP is simple and helps to
   parallelize image filter quite easily once we identify the parts of the
   algorithm that operate independently.

2. Vector instructions:
   Until I implemented filters with Halide, I could not understand what CPU
   instrutions were really about.
   I am not going to pretend to be an expert in CPU optimization but this little
   paragraph should convince you why it is so interesting to apply vector
   instructions wherever possible.
   So, as a first approximation, a CPU vector instruction typically enables the
   programmer to perform arithmetic operations on small vectors in a single CPU
   cycle. Typically arithmetic operations such addition, multiplication and more
   can operate on 4D vectors. That is where we can observe additional 4x speed
   up or more if your CPU can process those operations on bigger vectors.

For more accurate and more comprehensive information, I will simply encourage
you to do your own research and share what you have learnt.

Like image filter, BLAS routines makes extensive use of the two types
parallelism on CPU platforms.

### Schedule 1

The obvious strategy is to start from the idea of separable convolution and
vectorize the convolution wherever possible.

```
kernel.compute_root();

conv_x
    .compute_root()
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;
conv_y
    .parallel(y)
    .vectorize(x, 32, Halide::TailStrategy::GuardWithIf)
    ;
```

Then you decide to compete with OpenCV and you
realize that it crushes your implementation performance by being 2x faster.

In fact, the optimal schedule is really not obvious in CPU as exposed in Halide
presentations. Until you dig into Halide publications, you start to understand
how much work and expertise it is to optimize a typical image processing filter
in Photoshop and took 3 months of hard work to call CPU vector intrinsics.

The first step to achieve this to divide the final convolved image into tiles of
64 by 64 pixels. The set of tiles can be seen as an input batch of many smaller
images. The output is another batch of image tiles with the same image sizes
(let's just assume that the image width and height are multiple of 64.)

Because each output image tile is independent of each other, we can calculate
smaller x-convolution and y-convolution. For each tile we can fit the
x-convolution in the CPU cache and it improves data locality in the memory
cache. Then we explot the CPU vector instructions to calculate convolution in
batch.

### Schedule 2: Parallel tile processing, vectorize, CPU cache fitting

```
// The schedule
kernel.compute_root();

conv_y  //
    .tile(x, y, xo, yo, xi, yi, 64, 64)
    .fuse(xo, yo, tile)
    .parallel(tile)
    .vectorize(xi, 4, Halide::TailStrategy::GuardWithIf)  //
    ;
conv_x
    .compute_at(conv_y, xi)                               //
    .vectorize(x, 4, Halide::TailStrategy::GuardWithIf)  //
    ;
```

#### Second-Guessing what Halide does
There is a lot to unpack here. Let's break it down bit by bit.

The line
```{cpp}
  conv_y.tile(x, y, xo, yo, xi, yi, 64, 64, Halide::TailStrategy::GuardWithIf)
      .fuse(xo, yo, tile_index)
      .parallel(tile_index);
      .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
```

translates in C++ as

This is according to my second-guessing:
```{cpp}
#pragma omp parallel for  // .parallel(y)
for (auto tile_index = 0; tile_index < T; ++T)
{
  // Process the tile (xo, yo).
  const auto yo = tile_index / T;
  const auto xo = tile_index % T;

#pragma omp parallel for
  for (auto yi = 0; yi < 64; ++yi)
  {
    for (auto xi = 0; xi < 64; ++yi)
    {
      const auto xi_begin = xo * 64 + xi;
      const auto xi_end = (xo + 1) * 64 + xi;

      // Recompose x and y.
      const auto y = yo * 64 + yi;
      const auto x = xo * 64 + xi;


      // conv_y(x, y) = sum(conv_x(x, y + k) * kernel(k));
      //
      // means:
      //
      // conv_y(x, y) = conv_x(x, y + 0) * kernel(0)
      //              + conv_x(x, y + 1) * kernel(1)
      //              + ...
      //              + conv_x(x, y + ksz -1) * kernel(ksz -1)
      //
      // Then with:
      // conv_x.compute_at(conv_y, x)
      //
      // Inferring
      https://halide-lang.org/docs/class_halide_1_1_func.html#a800cbcc3ca5e3d3fa1707f6e1990ec83.
      // The storage **has** to be 2D, if we unwrap conv_x
      float conv_x[ksz][ksz];

      // TODO: verify this.
      //
      // Vectorizable.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] = 0.f;
      // Vectorizable in xi? Unclear.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] += in[y][x + k] * kernel[k];

      // Vectorize in xi
      for (auto k = 0; k < ksz; ++ksz)
        conv[y][x] = 0.f;

      // Vectorizable in xi? Unclear.
      for (auto k = 0; k < ksz; ++ksz)
        conv_y[y][x] += conv_x[yi+k][xi] * kernel[k];
    }
  }

  conv_y =
}
```

To have a more definite answer, the best thing is to actually generate the
compiled code with

Second-guessing is not an option to get a serious understanding. So we need to
spend some time to learn how to actually read assembly code.

Halide developers has done a very good job to help us understand what assembly
code is generated mapping the assembly code to the pseudo code.

```{cpp}
# Generate code as follows
conv_y.compile_to_stmt("separable_conv_2d.stmt.html", {}, Halide::HTML);
```

Then it becomes clear that the convolution operation is implemented by batch
where we calculate 4, 8 or 16 convolved values at the same time by repeating
the vectorized fused multiply-add `fmla.4s` operation.


References:
- https://diveintosystems.org/book/C9-ARM64/common.html
- https://developer.arm.com/documentation/
