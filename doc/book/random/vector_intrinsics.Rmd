# Super Fast Separable Convolutions

Sounds boring? I promise you this is going to be way more interesting than you
might think. There's quite a bit interesting things to learn.

Story time before we dive into an existing example.

I applied for a C++ technical leadership role once at some company. I was
considered for the role after a preliminary behavioral screening by the CTO. He
then told me who I would be interviewing with in the next interviews. A few days
later, he called me back for a quick chat. He ended up telling me they would not
move forward after examining sara's image processing code. Without really
explaining why, I guessed that probably one of their senior engineers
disqualified me as he did care more about my ability in understanding and
manipulating CPU vector instructions.

The CTO profusedly apologized to me by saying that I certainly was gifted but my
image processing code was very slow because of my naive implementation. It did
sound unfair to me.

I have mostly an applied math background, so it sounded unfair to me. It did
made me feel that you are never enough whatever you achieve. Like you do need to
know every single aspect of engineering from high level to low level. In
hindsight I would not have been happy with the job anyways. Still frustrating...
In that moment, I was telling myself: what can you do when you are already being
shut the door? Going by the same logic, David Lowe showcased his SIFT research
code, he would not qualify for the job either: I learnt from studying his code.

Right, back to the topic: today how can we achieve that? Nowadays, we have some
answers with Halide. And we can do it very elegantly without coding in assembly
directly.

The main beauty with Halide is that you can decouple (1) the algorithm: the
separable convolution and (2) the scheduling strategy, i.e., how do you make it
as fast as the baseline: OpenCV's Gaussian blur?

Halide can check and provide guarantees that your algorithm remains correct for
the schedule you are implementing.

Another beauty with Halide is that it abstracts CPU, GPU intrinsics for you in a
generic C++ code for a wide range of platforms. You don't have to be an expert
in CPU vector intrinsics, but you do need to know the scheduling strategies to
say optimize the speed at which your convolutional operations run. So unless you
read publications and the presentations, it is hard for the layman or the novice
to know what works and what doesn't.


## Naive Implementation of the Gaussian blur in C.

Right, myself more than 15 years ago, let's imagine that I am a fresh graduate
with a Master's degree looking for an internship. I was a bit aware of CPU
registers but I can't see how it would fit in the grand scheme of things. I've
heard of vector instructions but never really. As an avid StackOverflow reader,
I have been told the big lie: don't prematurely optimize and just let the
compiler do its job. And just make things work.

I did learn the Gaussian filter is separable. Don't write the 2D convolution,
but exploit its separable properties. With the innocence of my youth, I am
proudly exhibiting a naive implementation in C++ in my Sara repo, something
along the lines:

```{Rcpp}
auto conv_x(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto xk = x - r + k;

        // Check the boundary conditions.
        if (xk < 0)
          xk = 0;
        else if (xk >= w)
          xk = w - 1;

        // Accumulate.
        const auto xy = y * w + xk;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}

auto conv_y(const float *in, const float *kernel, float *out,
            const int w, const int h, const int ksz) -> void
{
  const auto r = ksz / 2;
  for (auto y = 0; y < h; ++y)
  {
    for (auto x = 0; x < w; ++x)
    {
      auto val = 0.f;
      for (auto k = 0; k < ksz; ++k)
      {
        auto yk = y - r + k;

        // Check the boundary conditions.
        if (yk < 0)
          yk = 0;
        else if (yk >= h)
          yk = h - 1;

        // Accumulate.
        const auto xy = yk * w + x;
        val += in[xy] * kernel[k];
      }

      out[y * w + x] = val;
    }
  }
}
```

I write unit tests, validate on synthetic tests, check the boundary conditions
and try it on a real image with a Gaussian kernel. I am happy it works
reasonably fast when compiling in Release mode on Visual Studio. Job done! I am
proudly showcasing my code on GitHub. Never complained about it as I bothered
about real-time issues as later I learnt about in CUDA and would write in CUDA
anyways.


## 2D separable convolutional operation.

Many years later, the CTO and his minion tells you that you are not good enough
without elaborating what he was expecting. OK, let's make it better!

How? I vaguely understand you have to write the code with CPU intrinsics? Should
I write in C-style or in ASM. How to do it with limited bandwidth after
finishing your day job and wanting to learn?

Use Halide! Halide is very elegant language. Here is how we could rewrite the 2D
separable convolution.

```
#include <Halide.h>

auto x = Halide::Var{"x"};
auto y = Halide::Var{"y"};

auto xo = Halide::Var{"xo"};
auto yo = Halide::Var{"yo"};

auto xi = Halide::Var{"xi"};
auto yi = Halide::Var{"yi"};
auto tile = Halide::Var{"tile"};

auto input = Halide::Func{"input"};
auto kernel = Halide::Func{"kernel"};

input(x, y) = x + y;
kernel(x) = Halide::exp(-x);

auto conv_x = Halide::Func{"conv_x"};
auto conv_y = Halide::Func{"conv_y"};

const auto ksz = 20;
auto k = Halide::RDom(0, ksz, "k");

// The algorithm
conv_x(x, y) = Halide::sum(input(x + k - ksz / 2, y) * kernel(k), "conv_x");
conv_y(x, y) = Halide::sum(conv_x(x, y + k - ksz / 2) * kernel(k), "conv_y");
```

The optimal scheduling is not that obvious in CPU.

The first obvious strategy to do is to calculate the x-convolution first. Then
calculate the transpose of the y-convoluved map and transpose it back to obtain
the final results.

We can do a lot better by dividing the final convolved image into tiles of 64
by 64 pixels. Each image block is like a smaller independent image on which we
perform the 2D separable convolution.

Scheduling
```
// The schedule
kernel.compute_root();

conv_y  //
    .tile(x, y, xo, yo, xi, yi, 64, 64)
    .fuse(xo, yo, tile)
    .parallel(tile)
    // .parallel(yo)
    .vectorize(xi, 4, Halide::TailStrategy::GuardWithIf)  //
    ;
conv_x  //
        // .store_at(conv_y, tile)
    .compute_at(conv_y, xi)                               //
    .vectorize(x, 4, Halide::TailStrategy::GuardWithIf)  //
    ;
```

There is a lot to unpack here. Let's break it down bit by bit.

The line
```
  conv_y.tile(x, y, xo, yo, xi, yi, 64, 64, Halide::TailStrategy::GuardWithIf)
      .fuse(xo, yo, tile_index)
      .parallel(tile_index);
      .vectorize(xi, 16, Halide::TailStrategy::GuardWithIf)
```

translates in C++ as

This is according to my second-guessing:
```
#pragma omp parallel for  // .parallel(y)
for (auto tile_index = 0; tile_index < T; ++T)
{
  // Process the tile (xo, yo).
  const auto yo = tile_index / T;
  const auto xo = tile_index % T;

#pragma omp parallel for
  for (auto yi = 0; yi < 64; ++yi)
  {
    for (auto xi = 0; xi < 64; ++yi)
    {
      const auto xi_begin = xo * 64 + xi;
      const auto xi_end = (xo + 1) * 64 + xi;

      // Recompose x and y.
      const auto y = yo * 64 + yi;
      const auto x = xo * 64 + xi;


      // conv_y(x, y) = sum(conv_x(x, y + k) * kernel(k));
      //
      // means:
      //
      // conv_y(x, y) = conv_x(x, y + 0) * kernel(0)
      //              + conv_x(x, y + 1) * kernel(1)
      //              + ...
      //              + conv_x(x, y + ksz -1) * kernel(ksz -1)
      //
      // Then with:
      // conv_x.compute_at(conv_y, x)
      //
      // Inferring
      https://halide-lang.org/docs/class_halide_1_1_func.html#a800cbcc3ca5e3d3fa1707f6e1990ec83.
      // The storage **has** to be 2D, if we unwrap conv_x
      float conv_x[ksz][ksz];

      // TODO: verify this.
      //
      // Vectorizable.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] = 0.f;
      // Vectorizable in xi? Unclear.
      for (auto k = 0; k < ksz; ++k)
        conv_x[yi][xi] += in[y][x + k] * kernel[k];

      // Vectorize in xi
      for (auto k = 0; k < ksz; ++ksz)
        conv[y][x] = 0.f;

      // Vectorizable in xi? Unclear.
      for (auto k = 0; k < ksz; ++ksz)
        conv_y[y][x] += conv_x[yi+k][xi] * kernel[k];
    }
  }

  conv_y =
}
```

To have a more definite answer, the best thing is to actually generate the
compiled code with

Second-guessing is not an option to get a serious understanding. So we need to
spend some time to learn how to actually read assembly code.

Halide developers has done a very good job to help us understand what assembly
code is generated mapping the assembly code to the pseudo code.

```
# Generate code as follows
conv_y.compile_to_stmt("separable_conv_2d.stmt.html", {}, Halide::HTML);
```

It becomes clear that convolution operation operates in batch via fma operation.


References:
- https://diveintosystems.org/book/C9-ARM64/common.html
- https://developer.arm.com/documentation/ddi0602/2023-12/SIMD-FP-Instructions/FMLA--vector---Floating-point-fused-Multiply-Add-to-accumulator--vector--?lang=en
